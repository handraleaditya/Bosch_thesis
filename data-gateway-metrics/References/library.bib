@article{Zebari2020,
   abstract = {<p>Due to sharp increases in data dimensions, working on every data mining or machine learning (ML) task requires more efficient techniques to get the desired results. Therefore, in recent years, researchers have proposed and developed many methods and techniques to reduce the high dimensions of data and to attain the required accuracy. To ameliorate the accuracy of learning features as well as to decrease the training time dimensionality reduction is used as a pre-processing step, which can eliminate irrelevant data, noise, and redundant features. Dimensionality reduction (DR) has been performed based on two main methods, which are feature selection (FS) and feature extraction (FE). FS is considered an important method because data is generated continuously at an ever-increasing rate; some serious dimensionality problems can be reduced with this method, such as decreasing redundancy effectively, eliminating irrelevant data, and ameliorating result comprehensibility. Moreover, FE transacts with the problem of finding the most distinctive, informative, and decreased set of features to ameliorate the efficiency of both the processing and storage of data. This paper offers a comprehensive approach to FS and FE in the scope of DR. Moreover, the details of each paper, such as used algorithms/approaches, datasets, classifiers, and achieved results are comprehensively analyzed and summarized. Besides, a systematic discussion of all of the reviewed methods to highlight authors' trends, determining the method(s) has been done, which significantly reduced computational time, and selecting the most accurate classifiers. As a result, the different types of both methods have been discussed and analyzed the findings.  </p>},
   author = {Rizgar Zebari and Adnan Abdulazeez and Diyar Zeebaree and Dilovan Zebari and Jwan Saeed},
   doi = {10.38094/jastt1224},
   issn = {2708-0757},
   issue = {2},
   journal = {Journal of Applied Science and Technology Trends},
   month = {5},
   pages = {56-70},
   title = {A Comprehensive Review of Dimensionality Reduction Techniques for Feature Selection and Feature Extraction},
   volume = {1},
   year = {2020},
}
@article{Ayesha2020,
   abstract = {The recent developments in the modern data collection tools, techniques, and storage capabilities are leading towards huge volume of data. The dimensions of data indicate the number of features that have been measured for each observation. It has become a challenging task to analyze high dimensional data. Different dimensionality reduction techniques are available in literature to eliminate irrelevant and redundant features. Selection of an appropriate dimension reduction technique can help to enhance the processing speed and reduce the time and effort required to extract valuable information. This paper presents the state-of-the art dimensionality reduction techniques and their suitability for different types of data and application areas. Furthermore, the issues of dimensionality reduction techniques have been highlighted that can affect the accuracy and relevance of results.},
   author = {Shaeela Ayesha and Muhammad Kashif Hanif and Ramzan Talib},
   doi = {10.1016/J.INFFUS.2020.01.005},
   issn = {1566-2535},
   journal = {Information Fusion},
   keywords = {Dimensionality reduction,Features,High dimensional data,Linear techniques,Nonlinear techniques},
   month = {7},
   pages = {44-58},
   publisher = {Elsevier},
   title = {Overview and comparative study of dimensionality reduction techniques for high dimensional data},
   volume = {59},
   year = {2020},
}
@article{Espadoto2021,
   abstract = {Dimensionality reduction methods, also known as projections, are frequently used in multidimensional data exploration in machine learning, data science, and information visualization. Tens of such techniques have been proposed, aiming to address a wide set of requirements, such as ability to show the high-dimensional data structure, distance or neighborhood preservation, computational scalability, stability to data noise and/or outliers, and practical ease of use. However, it is far from clear for practitioners how to choose the best technique for a given use context. We present a survey of a wide body of projection techniques that helps answering this question. For this, we characterize the input data space, projection techniques, and the quality of projections, by several quantitative metrics. We sample these three spaces according to these metrics, aiming at good coverage with bounded effort. We describe our measurements and outline observed dependencies of the measured variables. Based on these results, we draw several conclusions that help comparing projection techniques, explain their results for different types of data, and ultimately help practitioners when choosing a projection for a given context. Our methodology, datasets, projection implementations, metrics, visualizations, and results are publicly open, so interested stakeholders can examine and/or extend this benchmark.},
   author = {Mateus Espadoto and Rafael M. Martins and Andreas Kerren and Nina S.T. Hirata and Alexandru C. Telea},
   doi = {10.1109/TVCG.2019.2944182},
   issn = {19410506},
   issue = {3},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   keywords = {Dimensionality reduction,benchmarking,design space,quality metrics,quantitative analysis},
   month = {3},
   pages = {2153-2173},
   pmid = {31567092},
   publisher = {IEEE Computer Society},
   title = {Toward a Quantitative Survey of Dimension Reduction Techniques},
   volume = {27},
   year = {2021},
}
@article{Heulot2017,
   abstract = {Multidimensional scaling allows visualizing high-dimensional data as 2D maps with the premise that insights in 2D reveal valid information in high-dimensions. However, the resulting projections suffer from artifacts such as bad local neighborhood preservation and clusters tearing. Interactively coloring the projection according to the discrepancy between original proximities relative to a reference item reveals these artifacts, but it is not clear if conveying these proximities using color and displaying only local information really helps the visual analysis of projections. We conducted a controlled experiment to investigate the relevance of this interactive technique to help the visual analysis of any projection regardless its quality. We compared the bare projection to the interactive coloring of the original proximities on different visual analysis tasks involving outliers and clusters. Results indicate that the interactive coloring is worthwhile for local tasks as it is significantly robust to projection artifacts whereas the projection is not. However this interactive technique does not help significantly for visual clustering tasks for that projections already give a suitable overview.},
   author = {Nicolas Heulot and Jean-Daniel Fekete and Michael Aupetit},
   month = {5},
   title = {Visualizing Dimensionality Reduction Artifacts: An Evaluation},
   url = {http://arxiv.org/abs/1705.05283},
   year = {2017},
}
@article{McInnes2018,
   abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
   author = {Leland McInnes and John Healy and James Melville},
   month = {2},
   title = {UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
   url = {http://arxiv.org/abs/1802.03426},
   year = {2018},
}
@article{Buja1996,
   abstract = {We propose a rudimentary taxonomy of interactive data visualization based on a triad of data analytic tasks: finding Gestalt, posing queries, and making comparisons. These tasks are supported by three classes of interactive view manipulations: focusing, linking, and arranging views. This discussion extends earlier work on the principles of focusing and linking and sets them on a firmer base. Next, we give a high-level introduction to a particular system for multivariate data visualization - XGobi. This introduction is not comprehensive but emphasizes XGobi tools that are examples of focusing, linking, and arranging views; namely, high-dimensional projections, linked scatterplot brushing, and matrices of conditional plots. Finally, in a series of case studies in data visualization, we show the powers and limitations of particular focusing, linking, and arranging tools. The discussion is dominated by high-dimensional projections that form an extremely well-developed part of XGobi. Of particular interest are the illustration of asymptotic normality of high-dimensional projections (a theorem of Diaconis and Freedman), the use of high-dimensional cubes for visualizing factorial experiments, and a method for interactively generating matrices of conditional plots with high-dimensional projections. Although there is a unifying theme to this article, each section - in particular the case studies - can be read separately.},
   author = {Andreas Buja and Dianne Cook and Deborah F. Swayne},
   doi = {10.2307/1390754},
   issn = {10618600},
   issue = {1},
   journal = {Journal of Computational and Graphical Statistics},
   month = {3},
   pages = {78},
   publisher = {JSTOR},
   title = {Interactive High-Dimensional Data Visualization},
   volume = {5},
   year = {1996},
}
@article{Yin2007,
   abstract = {Dimensionality reduction and data visualization are useful and important processes in pattern recognition. Many techniques have been developed in the recent years. The self-organizing map (SOM) can be an efficient method for this purpose. This paper reviews recent advances in this area and related approaches such as multidimensional scaling (MDS), nonlinear PCA, principal manifolds, as well as the connections of the SOM and its recent variant, the visualization induced SOM (ViSOM), with these approaches. The SOM is shown to produce a quantized, qualitative scaling and while the ViSOM a quantitative or metric scaling and approximates principal curve/surface. The SOM can also be regarded as a generalized MDS to relate two metric spaces by forming a topological mapping between them. The relationships among various recently proposed techniques such as ViSOM, Isomap, LLE, and eigenmap are discussed and compared. © 2007 Institute of Automation, Chinese Academy of Sciences.},
   author = {Hujun Yin},
   doi = {10.1007/S11633-007-0294-Y},
   issn = {14768186},
   issue = {3},
   journal = {International Journal of Automation and Computing},
   keywords = {Dimensionality reduction,Multidimensional scaling,Nonlinear PCA,Nonlinear data projection,Principal manifold,Self-organizing maps},
   month = {7},
   pages = {294-303},
   title = {Nonlinear dimensionality reduction and data visualization: A review},
   volume = {4},
   year = {2007},
}
@article{Tang2016,
   abstract = {We study the problem of visualizing large-scale and high-dimensional data in a low-dimensional (typically 2D or 3D) space. Much success has been reported recently by techniques that first compute a similarity structure of the data points and then project them into a low-dimensional space with the structure preserved. These two steps suffer from considerable computational costs, preventing the state-of-the-art methods such as the t-SNE from scaling to large-scale and high-dimensional data (e.g., millions of data points and hundreds of dimensions). We propose the LargeVis, a technique that first constructs an accurately approximated K-nearest neighbor graph from the data and then layouts the graph in the low-dimensional space. Comparing to t-SNE, LargeVis significantly reduces the computational cost of the graph construction step and employs a principled probabilistic model for the visualization step, the objective of which can be effectively optimized through asynchronous stochastic gradient descent with a linear time complexity. The whole procedure thus easily scales to millions of high-dimensional data points. Experimental results on real-world data sets demonstrate that the LargeVis outperforms the state-of-the-art methods in both efficiency and effectiveness. The hyper-parameters of LargeVis are also much more stable over different data sets.},
   author = {Jian Tang and Jingzhou Liu and Ming Zhang and Qiaozhu Mei},
   doi = {10.1145/2872427.2883041},
   month = {4},
   pages = {287-297},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Visualizing Large-scale and High-dimensional Data},
   year = {2016},
}
@article{Gisbrecht2015,
   abstract = {In this overview, commonly used dimensionality reduction techniques for data visualization and their properties are reviewed. Thereby, the focus lies on an intuitive understanding of the underlying mathematical principles rather than detailed algorithmic pipelines. Important mathematical properties of the technologies are summarized in the tabular form. The behavior of representative techniques is demonstrated for three benchmarks, followed by a short discussion on how to quantitatively evaluate these mappings. In addition, three currently active research topics are addressed: how to devise dimensionality reduction techniques for complex non-vectorial data sets, how to easily shape dimensionality reduction techniques according to the users preferences, and how to device models that are suited for big data sets.},
   author = {Andrej Gisbrecht and Barbara Hammer},
   doi = {10.1002/WIDM.1147},
   issn = {19424795},
   issue = {2},
   journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
   month = {3},
   pages = {51-73},
   publisher = {Wiley-Blackwell},
   title = {Data visualization by nonlinear dimensionality reduction},
   volume = {5},
   year = {2015},
}
@article{Fodor2002,
   abstract = {This paper, we assume that we have n observations, each being a realization of the p- dimensional random variable x = (x 1 , . . . , x p with mean E(x) = = 1 , . . . , p and covariance matrix E(x )(x = pp . We denote such an observation matrix by X = i,j : 1 p, 1 n. If i and i = (i,i) denote the mean and the standard deviation of the ith random variable, respectively, then we will often standardize the observations x i,j by (x i,j i i , where i = x i = 1/n j=1 x i,j , and i = 1/n j=1 (x i,j x i},
   author = {Imola K Fodor},
   doi = {10.2172/15002155},
   issue = {1},
   journal = {Library},
   pages = {1-18},
   title = {A survey of dimension reduction techniques},
   volume = {18},
   url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.122.201&amp;rep=rep1&amp;type=pdf},
   year = {2002},
}
@article{Espadoto2019,
   abstract = {Visualizing decision boundaries of modern machine learning classifiers can notably help in classifier design, testing, and fine-tuning. Dense maps are a very recent method that overcomes the key sparsity-related limitation of scatterplots for this task. However, the trustworthiness of dense maps heavily depends on the underlying dimensionality-reduction (DR) techniques they use. We design and perform a detailed study aimed at finding the best DR techniques to use when creating trustworthy dense maps, by studying a large collection of 28 DR algorithms, 4 classifiers, and 2 datasets from a real-world challenging classification problem. Our results show how one can pick suitable DR algorithms to create dense maps that help understanding classifier behavior.},
   author = {Mateus Espadoto and Francisco Caio M. Rodrigues and Alexandru C. Telea},
   doi = {10.5220/0007260800280038},
   isbn = {9789897583544},
   journal = {VISIGRAPP 2019 - Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
   keywords = {Dimensionality reduction,Image-based visualization,Machine learning},
   pages = {28-38},
   publisher = {SciTePress},
   title = {Visual analytics of multidimensional projections for constructing classifier decision boundary maps},
   volume = {3},
   year = {2019},
}
@article{Nguyen2019,
   author = {Lan Huong Nguyen and Susan Holmes},
   doi = {10.1371/JOURNAL.PCBI.1006907},
   issn = {1553-7358},
   issue = {6},
   journal = {PLOS Computational Biology},
   keywords = {Aspect ratio,Data visualization,Eigenvalues,Kernel methods,Linear discriminant analysis,Phenols,Principal component analysis,Wine},
   month = {6},
   pages = {e1006907},
   pmid = {31220072},
   publisher = {Public Library of Science},
   title = {Ten quick tips for effective dimensionality reduction},
   volume = {15},
   url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006907},
   year = {2019},
}
@article{Ting2018,
   abstract = {We develop theory for nonlinear dimensionality reduction (NLDR). A number of NLDR methods have been developed, but there is limited understanding of how these methods work and the relationships between them. There is limited basis for using existing NLDR theory for deriving new algorithms. We provide a novel framework for analysis of NLDR via a connection to the statistical theory of linear smoothers. This allows us to both understand existing methods and derive new ones. We use this connection to smoothing to show that asymptotically, existing NLDR methods correspond to discrete approximations of the solutions of sets of differential equations given a boundary condition. In particular, we can characterize many existing methods in terms of just three limiting differential operators and boundary conditions. Our theory also provides a way to assert that one method is preferable to another; indeed, we show Local Tangent Space Alignment is superior within a class of methods that assume a global coordinate chart defines an isometric embedding of the manifold.},
   author = {Daniel Ting and Michael I. Jordan},
   month = {3},
   title = {On Nonlinear Dimensionality Reduction, Linear Smoothing and Autoencoding},
   url = {http://arxiv.org/abs/1803.02432},
   year = {2018},
}
@article{Cao2017,
   abstract = {t-Distributed Stochastic Neighbor Embedding (t-SNE) is one of the most widely used dimensionality reduction methods for data visualization, but it has a perplexity hyperparameter that requires manual selection. In practice, proper tuning of t-SNE perplexity requires users to understand the inner working of the method as well as to have hands-on experience. We propose a model selection objective for t-SNE perplexity that requires negligible extra computation beyond that of the t-SNE itself. We empirically validate that the perplexity settings found by our approach are consistent with preferences elicited from human experts across a number of datasets. The similarities of our approach to Bayesian information criteria (BIC) and minimum description length (MDL) are also analyzed.},
   author = {Yanshuai Cao and Luyu Wang},
   month = {8},
   title = {Automatic Selection of t-SNE Perplexity},
   url = {http://arxiv.org/abs/1708.03229},
   year = {2017},
}
@article{Sorzano2014,
   abstract = {Experimental life sciences like biology or chemistry have seen in the recent decades an explosion of the data available from experiments. Laboratory instruments become more and more complex and report hundreds or thousands measurements for a single experiment and therefore the statistical methods face challenging tasks when dealing with such high dimensional data. However, much of the data is highly redundant and can be efficiently brought down to a much smaller number of variables without a significant loss of information. The mathematical procedures making possible this reduction are called dimensionality reduction techniques; they have widely been developed by fields like Statistics or Machine Learning, and are currently a hot research topic. In this review we categorize the plethora of dimension reduction techniques available and give the mathematical insight behind them.},
   author = {C. O. S. Sorzano and J. Vargas and A. Pascual Montano},
   month = {3},
   title = {A survey of dimensionality reduction techniques},
   url = {http://arxiv.org/abs/1403.2877},
   year = {2014},
}
@article{Abdulhammed2019,
   abstract = {The security of networked systems has become a critical universal issue that influences individuals, enterprises and governments. The rate of attacks against networked systems has increased dramatically, and the tactics used by the attackers are continuing to evolve. Intrusion detection is one of the solutions against these attacks. A common and effective approach for designing Intrusion Detection Systems (IDS) is Machine Learning. The performance of an IDS is significantly improved when the features are more discriminative and representative. This study uses two feature dimensionality reduction approaches: (i) Auto-Encoder (AE): an instance of deep learning, for dimensionality reduction, and (ii) Principle Component Analysis (PCA). The resulting low-dimensional features from both techniques are then used to build various classifiers such as Random Forest (RF), Bayesian Network, Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) for designing an IDS. The experimental findings with low-dimensional features in binary and multi-class classification show better performance in terms of Detection Rate (DR), F-Measure, False Alarm Rate (FAR), and Accuracy. This research effort is able to reduce the CICIDS2017 dataset&rsquo;s feature dimensions from 81 to 10, while maintaining a high accuracy of 99.6% in multi-class and binary classification. Furthermore, in this paper, we propose a Multi-Class Combined performance metric     C o m b i n e  d  M c       with respect to class distribution to compare various multi-class and binary classification systems through incorporating FAR, DR, Accuracy, and class distribution parameters. In addition, we developed a uniform distribution based balancing approach to handle the imbalanced distribution of the minority class instances in the CICIDS2017 network intrusion dataset.},
   author = {Razan Abdulhammed and Hassan Musafer and Ali Alessa and Miad Faezipour and Abdelshakour Abuzneid},
   doi = {10.3390/ELECTRONICS8030322},
   issn = {2079-9292},
   issue = {3},
   journal = {Electronics 2019, Vol. 8, Page 322},
   keywords = {Dimensionality Reduction,Intrusion Detection System (IDS),Principle Component Analysis (PCA),Sparse Auto Encoder (SAE),Uniform Distribution Based Balancing (UDBB)},
   month = {3},
   pages = {322},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Features Dimensionality Reduction Approaches for Machine Learning Based Network Intrusion Detection},
   volume = {8},
   url = {https://www.mdpi.com/2079-9292/8/3/322/htm https://www.mdpi.com/2079-9292/8/3/322},
   year = {2019},
}
@article{Abe2010,
   author = {Shigeo Abe},
   doi = {10.1007/978-1-84996-098-4_7},
   pages = {331-341},
   title = {Feature Selection and Extraction},
   url = {http://link.springer.com/10.1007/978-1-84996-098-4_7},
   year = {2010},
}
@article{Anowar2021,
   abstract = {Feature Extraction Algorithms (FEAs) aim to address the curse of dimensionality that makes machine learning algorithms incompetent. Our study conceptually and empirically explores the most representative FEAs. First, we review the theoretical background of many FEAs from different categories (linear vs. nonlinear, supervised vs. unsupervised, random projection-based vs. manifold-based), present their algorithms, and conduct a conceptual comparison of these methods. Secondly, for three challenging binary and multi-class datasets, we determine the optimal sets of new features and assess the quality of the various transformed feature spaces in terms of statistical significance and power analysis, and the FEA efficacy in terms of classification accuracy and speed.},
   author = {Farzana Anowar and Samira Sadaoui and Bassant Selim},
   doi = {10.1016/J.COSREV.2021.100378},
   issn = {1574-0137},
   journal = {Computer Science Review},
   keywords = {Classification accuracy,Correlation metrics,Data quality,Dimension reduction,High-dimensional datasets,Optimal set of features,Run-time},
   month = {5},
   pages = {100378},
   publisher = {Elsevier},
   title = {Conceptual and empirical comparison of dimensionality reduction algorithms (PCA, KPCA, LDA, MDS, SVD, LLE, ISOMAP, LE, ICA, t-SNE)},
   volume = {40},
   year = {2021},
}
@article{Jindal2017,
   abstract = {Progress in digital data acquisition and storage technology has resulted in exponential growth in high dimensional data. Removing redundant and irrelevant features from this high-dimensional data helps in improving mining performance and comprehensibility and increasing learning accuracy. Feature selection and feature extraction techniques as a preprocessing step are used for reducing data dimensionality. This paper analyses some existing popular feature selection and feature extraction techniques and addresses benefits and challenges of these algorithms which would be beneficial for beginners..},
   author = {Priyanka Jindal and Dharmender Kumar},
   doi = {10.5120/IJCA2017915260},
   issue = {2},
   journal = {International Journal of Computer Applications},
   month = {9},
   pages = {42-46},
   publisher = {Foundation of Computer Science},
   title = {A Review on Dimensionality Reduction Techniques},
   volume = {173},
   year = {2017},
}
@article{Ghojogh2019,
   abstract = {Pattern analysis often requires a pre-processing stage for extracting or selecting features in order to help the classification, prediction, or clustering stage discriminate or represent the data in a better way. The reason for this requirement is that the raw data are complex and difficult to process without extracting or selecting appropriate features beforehand. This paper reviews theory and motivation of different common methods of feature selection and extraction and introduces some of their applications. Some numerical implementations are also shown for these methods. Finally, the methods in feature selection and extraction are compared.},
   author = {Benyamin Ghojogh and Maria N. Samad and Sayema Asif Mashhadi and Tania Kapoor and Wahab Ali and Fakhri Karray and Mark Crowley},
   month = {5},
   title = {Feature Selection and Feature Extraction in Pattern Analysis: A Literature Review},
   url = {http://arxiv.org/abs/1905.02845},
   year = {2019},
}
@article{,
   abstract = {The prediction of cardiac disease helps practitioners make more accurate decisions regarding patients' health. Therefore, the use of machine learning (ML) is a solution to reduce and understand the symptoms related to heart disease. The aim of this work is the proposal of a dimensionality reduction method and finding features of heart disease by applying a feature selection technique. The information used for this analysis was obtained from the UCI Machine Learning Repository called Heart Disease. The dataset contains 74 features and a label that we validated by six ML classifiers. Chi-square and principal component analysis (CHI-PCA) with random forests (RF) had the highest accuracy, with 98.7% for Cleveland, 99.0% for Hungarian, and 99.4% for Cleveland-Hungarian (CH) datasets. From the analysis, ChiSqSelector derived features of anatomical and physiological relevance, such as cholesterol, highest heart rate, chest pain, features related to ST depression, and heart vessels. The experimental results proved that the combination of chi-square with PCA obtains greater performance in most classifiers. The usage of PCA directly from the raw data computed lower results and would require greater dimensionality to improve the results.},
   author = {Anna Karen Gárate-Escamila and Amir Hajjam El Hassani and Emmanuel Andrès},
   doi = {10.1016/J.IMU.2020.100330},
   issn = {2352-9148},
   journal = {Informatics in Medicine Unlocked},
   keywords = {Apache spark,Feature selection,Heart disease,Machine learning,PCA},
   month = {1},
   pages = {100330},
   publisher = {Elsevier},
   title = {Classification models for heart disease prediction using feature selection and PCA},
   volume = {19},
   year = {2020},
}
@article{Wickramasinghe2021,
   abstract = {Efficient modeling of high-dimensional data requires extracting only relevant dimensions through feature learning. Unsupervised feature learning has gained tremendous attention due to its unbiased approach, no need for prior knowledge or expensive manual processing, and ability to handle exponential data growth. Deep Autoencoder (AE) is a state-of-the-art deep neural network for unsupervised feature learning, which learns embedded-representations using a series of stacked layers. However, as the AE network gets deeper, these learned embedded-representations can deteriorate due to vanishing gradient, leading to performance degradation. This article presents ResNet Autoencoder (RAE) and its convolutional version (C-RAE) for unsupervised feature learning. The advantage of RAE and C-RAE is that it enables the user to add residual connections for increased network capacity without incurring the cost of degradation for unsupervised feature learning compared to standard AEs. While RAE and C-RAE inherit all the advantages of AEs, such as automated non-linear feature extraction and unsupervised learning, they also allow users to design larger networks without adverse effects on feature learning performance. We performed classification on learned embedded-representation to evaluate RAE and C-RAE. RAE and C-RAE were compared against AEs on MNIST, Fashion MNIST, and CIFAR10 datasets. When increasing the number of layers, C-RAE outperformed AE by showing significantly lower performance degradation of classification accuracy (less than 3%) compared to AE (33% to 65%). Further, C-RAE exhibited higher mean accuracy and lower variance of accuracy than standard AE. When comparing RAE and C-RAE with widely used feature learning methods (Convolutional AE, PCA, ICA, LLE, Factor Analysis, and SVD), C-RAE showed the highest accuracy.},
   author = {Chathurika S. Wickramasinghe and Daniel L. Marino and Milos Manic},
   doi = {10.1109/ACCESS.2021.3064819},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Deep learning,ResNet,autoencoders,classification,deep embedded classification,dimension reduction,feature learning,unsupervised learning},
   pages = {40511-40520},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {ResNet Autoencoders for Unsupervised Feature Learning from High-Dimensional Data: Deep Models Resistant to Performance Degradation},
   volume = {9},
   year = {2021},
}
@article{Aljalbout2018,
   abstract = {Clustering methods based on deep neural networks have proven promising for clustering real-world data because of their high representational power. In this paper, we propose a systematic taxonomy of clustering methods that utilize deep neural networks. We base our taxonomy on a comprehensive review of recent work and validate the taxonomy in a case study. In this case study, we show that the taxonomy enables researchers and practitioners to systematically create new clustering methods by selectively recombining and replacing distinct aspects of previous methods with the goal of overcoming their individual limitations. The experimental evaluation confirms this and shows that the method created for the case study achieves state-of-the-art clustering quality and surpasses it in some cases.},
   author = {Elie Aljalbout and Vladimir Golkov and Yawar Siddiqui and Maximilian Strobel and Daniel Cremers},
   month = {1},
   title = {Clustering with Deep Learning: Taxonomy and New Methods},
   url = {http://arxiv.org/abs/1801.07648},
   year = {2018},
}
@article{Jia2022,
   abstract = {As basic research, it has also received increasing attention from people that the “curse of dimensionality” will lead to increase the cost of data storage and computing; it also influences the efficiency and accuracy of dealing with problems. Feature dimensionality reduction as a key link in the process of pattern recognition has become one hot and difficulty spot in the field of pattern recognition, machine learning and data mining. It is one of the most challenging research fields, which has been favored by most of the scholars’ attention. How to implement “low loss” in the process of feature dimension reduction, keep the nature of the original data, find out the best mapping and get the optimal low dimensional data are the keys aims of the research. In this paper, two-dimensionality reduction methods, feature selection and feature extraction, are introduced; the current mainstream dimensionality reduction algorithms are analyzed, including the method for small sample and method based on deep learning. For each algorithm, examples of their application are given and the advantages and disadvantages of these methods are evaluated.},
   author = {Weikuan Jia and Meili Sun and Jian Lian and Sujuan Hou},
   doi = {10.1007/S40747-021-00637-X},
   issn = {2198-6053},
   issue = {3},
   journal = {Complex & Intelligent Systems 2022 8:3},
   keywords = {Complexity,Computational Intelligence,Data Structures and Information Theory,Feature selection},
   month = {1},
   pages = {2663-2693},
   publisher = {Springer},
   title = {Feature dimensionality reduction: a review},
   volume = {8},
   url = {https://link.springer.com/article/10.1007/s40747-021-00637-x},
   year = {2022},
}
@article{Lu2023,
   abstract = {The capability of learning and generalizing from very few samples successfully is a noticeable demarcation separating artificial intelligence and human intelligence. Despite the long history dated back to the early 2000s and the widespread attention in recent years with booming deep learning, few surveys for few sample learning (FSL) are available. We extensively study almost all papers of FSL spanning from the 2000s to now and provide a timely and comprehensive survey for FSL. In this survey, we review the evolution history and current progress on FSL, categorize FSL approaches into the generative model based and discriminative model based kinds in principle, and emphasize particularly on the meta learning based FSL approaches. We also summarize several recently emerging extensional topics of FSL and review their latest advances. Furthermore, we highlight the important FSL applications covering many research hotspots in computer vision, natural language processing, audio and speech, reinforcement learning and robotic, data analysis, etc. Finally, we conclude the survey with a discussion on promising trends in the hope of providing guidance and insights to follow-up researches.},
   author = {Jiang Lu and Pinghua Gong and Jieping Ye and Jianwei Zhang and Changshui Zhang},
   doi = {10.1016/j.patcog.2023.109480},
   issn = {00313203},
   journal = {Pattern Recognition},
   keywords = {Few sample learning,Few-shot learning,Learn to learn,Meta learning,Survey},
   month = {7},
   publisher = {Elsevier Ltd},
   title = {A survey on machine learning from few samples},
   volume = {139},
   year = {2023},
}
@article{Raymer2000,
   abstract = {Pattern recognition generally requires that objects be described in terms of a set of measurable features. The selection and quality of the features representing each pattern have a considerable bearing on the success of subsequent pattern classification. Feature extraction is the process of deriving new features from the original features in order to reduce the cost of feature measurement, increase classifier efficiency, and allow higher classification accuracy. Many current feature extraction techniques involve linear transformations of the original pattern vectors to new vectors of lower dimensionality. While this is useful for data visualization and increasing classification efficiency, it does not necessarily reduce the number of features that must be measured since each new feature may be a linear combination of all of the features in the original pattern vector. Here, we present a new approach to feature extraction in which feature selection, feature extraction, and classifier training are performed simultaneously using a genetic algorithm. The genetic algorithm optimizes a vector of feature weights, which are used to scale the individual features in the original pattern vectors in either a linear or a nonlinear fashion. A masking vector is also employed to perform simultaneous selection of a subset of the features. We employ this technique in combination with the k nearest neighbor classification rule, and compare the results with classical feature selection and extraction techniques, including sequential floating forward feature selection, and linear discriminant analysis. We also present results for the identification of favorable water-binding sites on protein surfaces, an important problem in biochemistry and drug design.},
   author = {Michael L. Raymer and William F. Punch and Erik D. Goodman and Leslie A. Kuhn and Anil K. Jain},
   doi = {10.1109/4235.850656},
   issn = {1089778X},
   issue = {2},
   journal = {IEEE Transactions on Evolutionary Computation},
   month = {7},
   pages = {164-171},
   title = {Dimensionality reduction using genetic algorithms},
   volume = {4},
   year = {2000},
}
@article{Huang2019,
   abstract = {High-dimensional data is ubiquitous in scientific research and industrial production fields. It brings a lot of information to people, at the same time, because of its sparse and redundancy, it als...},
   author = {Xuan Huang and Lei Wu and Yinsong Ye},
   doi = {10.1142/S0218001419500174},
   issn = {02180014},
   issue = {10},
   journal = {https://doi.org/10.1142/S0218001419500174},
   keywords = {Dimensionality reduction,feature extraction,feature selection,optimization,pattern recognition},
   month = {9},
   publisher = { World Scientific Publishing Company },
   title = {A Review on Dimensionality Reduction Techniques},
   volume = {33},
   year = {2019},
}
@article{Dash1997,
   abstract = {Dimensionality reduction is an important problem for efficient handling of large databases. Many feature selection methods exist for supervised data having class information. Little work has been done for dimensionality reduction of unsupervised data in which class information is not available. Principal Component Analysis (PCA) is often used. However, PCA creates new features. It is difficult to obtain intuitive understanding of the data using the new features only. In this paper we are concerned with the problem of determining and choosing the important original features for unsupervised data. Our method is based on the observation that removing an irrelevant feature from the feature set may not change the underlying concept of the data, but not so otherwise. We propose an entropy measure for ranking features, and conduct extensive experiments to show that our method is able to find the important features. Also it compares well with a similar feature ranking method (Relief) that requires class information unlike our method.},
   author = {M. Dash and H. Liu and J. Yao},
   doi = {10.1109/TAI.1997.632300},
   issn = {10636730},
   journal = {Proceedings of the International Conference on Tools with Artificial Intelligence},
   pages = {532-539},
   publisher = {IEEE},
   title = {Dimensionality reduction of unsupervised data},
   year = {1997},
}
@article{Zhong2017,
   abstract = {Deep neural networks can learn deep feature representation for hyperspectral image (HSI) interpretation and achieve high classification accuracy in different datasets. However, counterintuitively, the classification performance of deep learning models degrades as their depth increases. Therefore, we add identity mappings to convolutional neural networks for every two convolutional layers to build deep residual networks (ResNets). To study the influence of deep learning model size on HSI classification accuracy, this paper applied ResNets and CNNs with different depth and width using two challenging datasets. Moreover, we tested the effectiveness of batch normalization as a regularization method with different model settings. The experimental results demonstrate that ResNets mitigate the declining-accuracy effect and achieved promising classification performance with 10% and 5% training sample percentages for the University of Pavia and Indian Pines datasets, respectively. In addition, t-Distributed Stochastic Neighbor Embedding (t-SNE) provides a direct view of the extracted features through dimensionality reduction.},
   author = {Zilong Zhong and Jonathan Li and Lingfei Ma and Han Jiang and He Zhao},
   doi = {10.1109/IGARSS.2017.8127330},
   isbn = {9781509049516},
   journal = {International Geoscience and Remote Sensing Symposium (IGARSS)},
   keywords = {Deep residual networks,deep learning,hyperspectral image classification},
   month = {12},
   pages = {1824-1827},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Deep residual networks for hyperspectral image classification},
   volume = {2017-July},
   year = {2017},
}
@article{Xue2020,
   abstract = {With the continuous development of users' demands and network technology, more and more new network protocols emerge, which poses great challenges to network protocol classification and identification. An artificial intelligence method was used to explore autonomous classification and identification of unknown network protocols in this paper in order to reduce the time and labor cost of network protocol classification and identification. In this paper, firstly, the network traffic was converted into grayscale images, and through transfer learning, the Convolutional Neural Networks (CNN) pre-trained model was used to extract the protocol features, so as to reduce the time and the amount of labeled data needed for the artificial neural network training. Finally, with the improved unsupervised hybrid clustering algorithm based on T-SNE and K-means, the types and number of protocols were autonomously identified and the network traffic was classified simultaneously. In this way, we can identify unknown protocols without prior knowledge and the protocol identification adaptability for big data was also greatly improved. Experimental results show this method has high accuracy and robustness in identifying unknown network protocols.},
   author = {Jingliang Xue and Yingchun Chen and Ou Li and Fei Li},
   doi = {10.1088/1742-6596/1617/1/012071},
   issn = {1742-6596},
   issue = {1},
   journal = {Journal of Physics: Conference Series},
   month = {8},
   pages = {012071},
   publisher = {IOP Publishing},
   title = {Classification and identification of unknown network protocols based on CNN and T-SNE},
   volume = {1617},
   url = {https://iopscience.iop.org/article/10.1088/1742-6596/1617/1/012071 https://iopscience.iop.org/article/10.1088/1742-6596/1617/1/012071/meta},
   year = {2020},
}
@article{Zhang2020,
   abstract = { Narrow emission-line galaxies can be distinguished in the well-known BPT diagrams through narrow emission-line properties. However, there are no boundaries visible to the naked eye between type-2 active galactic nuclei (AGN) and H ii galaxies in BPT diagrams, besides the extreme dividing lines expected by theoretical photoionization models. Here, based on the powerful t-SNE technique applied to the local narrow emission-line galaxies in the Sloan Digital Sky Survey Data Release 15, type-2 AGN and H ii galaxies can be clearly separated in the t-SNE determined two-dimensional projected map, and then the dividing lines can be mathematically determined in BPT diagrams, leading to charming harmonization of the theoretical expectations and the actual results from real observed properties. The results not only provide an interesting and robust method to determine the dividing lines in BPT diagrams through the powerful t-SNE technique, but also lead to further confirmation on previously defined composite galaxies more efficiently classified in the BPT diagram of [O iii ]/H β versus [N ii ]/H α . },
   author = {XueGuang Zhang and Yanqiu Feng and Huan Chen and QiRong Yuan},
   doi = {10.3847/1538-4357/ABC478},
   issn = {0004-637X},
   issue = {2},
   journal = {The Astrophysical Journal},
   month = {12},
   pages = {97},
   publisher = {American Astronomical Society},
   title = {Powerful t-SNE Technique Leading to Clear Separation of Type-2 AGN and H ii Galaxies in BPT Diagrams},
   volume = {905},
   year = {2020},
}
@article{Steinhardt2020,
   abstract = {Large photometric surveys provide a rich source of observations of quiescent galaxies, including a surprisingly large population at z>1. However, identifying large, but clean, samples of quiescent galaxies has proven difficult because of their near-degeneracy with interlopers such as dusty, star-forming galaxies. We describe a new technique for selecting quiescent galaxies based upon t-distributed stochastic neighbor embedding (t-SNE), an unsupervised machine learning algorithm for dimensionality reduction. This t-SNE selection provides an improvement both over UVJ, removing interlopers which otherwise would pass color selection, and over photometric template fitting, more strongly towards high redshift. Due to the similarity between the colors of high- and low-redshift quiescent galaxies, under our assumptions t-SNE outperforms template fitting in 63% of trials at redshifts where a large training sample already exists. It also may be able to select quiescent galaxies more efficiently at higher redshifts than the training sample.},
   author = {Charles L. Steinhardt and John R. Weaver and Jack Maxfield and Iary Davidzon and Andreas L. Faisst and Dan Masters and Madeline Schemel and Sune Toft},
   doi = {10.3847/1538-4357/AB76BE},
   issn = {15384357},
   issue = {2},
   journal = {The Astrophysical Journal},
   month = {3},
   pages = {136},
   publisher = {American Astronomical Society},
   title = {A Method to Distinguish Quiescent and Dusty Star-forming Galaxies with Machine Learning},
   volume = {891},
   year = {2020},
}
@article{Traven2017,
   abstract = {Galah is an ongoing high-resolution spectroscopic survey with the goal of disentangling the formation history of the Milky Way, using the fossil remnants of disrupted star formation sites which are now dispersed around the Galaxy. It is targeting a randomly selected, magnitude limited ($V \leq 14$) sample of stars, with the goal of observing one million objects. To date, 300,000 spectra have been obtained. Not all of them are correctly processed by parameter estimation pipelines and we need to know about them. We present a semi-automated classification scheme which identifies different types of peculiar spectral morphologies, in an effort to discover and flag potentially problematic spectra and thus help to preserve the integrity of the survey's results. To this end we employ a recently developed dimensionality reduction technique t-SNE (t-distributed Stochastic Neighbour Embedding), which enables us to represent the complex spectral morphology in a two-dimensional projection map while still preserving the properties of the local neighbourhoods of spectra. We find that the majority (178,483) of the 209,533 Galah spectra considered in this study represents normal single stars, whereas 31,050 peculiar and problematic spectra with very diverse spectral features pertaining to 28,579 stars are distributed into 10 classification categories: Hot stars, Cool metal-poor giants, Molecular absorption bands, Binary stars, H$\alpha$/H$\beta$ emission, H$\alpha$/H$\beta$ emission superimposed on absorption, H$\alpha$/H$\beta$ P-Cygni, H$\alpha$/H$\beta$ inverted P-Cygni, Lithium absorption, and Problematic. Classified spectra with supplementary information are presented in the catalogue, indicating candidates for follow-up observations and population studies of the short-lived phases of stellar evolution.},
   author = {G. Traven and G. Matijevič and T. Zwitter and M. Žerjal and J. Kos and M. Asplund and J. Bland-Hawthorn and A. R. Casey and G. De Silva and K. Freeman and J. Lin and S. L. Martell and K. J. Schlesinger and S. Sharma and J. D. Simpson and D. B. Zucker and B. Anguiano and G. Da Costa and L. Duong and J. Horner and E. A. Hyde and P. R. Kafle and U. Munari and D. Nataf and C. A. Navin and W. Reid and Y.-S. Ting},
   doi = {10.3847/1538-4365/228/2/24},
   issn = {00670049},
   issue = {2},
   journal = {The Astrophysical Journal Supplement Series},
   month = {2},
   pages = {24},
   publisher = {American Astronomical Society},
   title = {The Galah Survey: Classification and Diagnostics with t-SNE Reduction of Spectral Information},
   volume = {228},
   year = {2017},
}
@article{,
   abstract = {Our understanding of galaxy evolution is derived from large surveys designed to maximize efficiency by only observing the minimum amount needed to infer properties for a typical galaxy. However, for a few percent of galaxies in every survey, these observations are insufficient and derived properties can be catastrophically wrong. Further, it is currently difficult or impossible to determine which objects have failed, so that these contaminate every study of galaxy properties. We develop a novel method to identify these objects by combining the astronomical codes that infer galaxy properties with the dimensionality reduction algorithm t-SNE, which groups similar objects to determine which inferred properties are out of place. This method provides an improvement for the COSMOS catalog, which already uses existing techniques for catastrophic error removal, and therefore should improve the quality of large catalogs and any studies that are sensitive to large redshift errors.},
   author = {Beryl Hovis-Afflerbach and Charles L. Steinhardt and Daniel Masters and Mara Salvato},
   doi = {10.3847/1538-4357/ABD329},
   issn = {0004-637X},
   issue = {2},
   journal = {The Astrophysical Journal},
   month = {2},
   pages = {148},
   publisher = {American Astronomical Society},
   title = {Identifying and Repairing Catastrophic Errors in Galaxy Properties Using Dimensionality Reduction},
   volume = {908},
   year = {2021},
}
@article{Balavand2022,
   abstract = {In complex data with high dimensions, the dimension reduction methods are used to increase accuracy and speed in the classification algorithms. Feature clustering methods have had a good performance in the selection of important features of data due to using clustering methods. The process of selecting important features of data is a challenge in feature clustering methods which has led to the creation of different algorithms with different performances. The combination of the clustering methods and metaheuristic algorithms, especially the kind of population-based algorithms, have had good results in most cases. In this paper, a new feature clustering method is proposed which is used as a dimension reduction in the classification of brain tumors in 900 magnetic resonance images (MRI). The classification algorithm includes three main steps: in the first step, the Google-Net and ResNet-18 methods have been used for feature extraction of MRI images. Due to the creation of many features using the Google-Net and ResNet-18 methods, a new proposed feature clustering is introduced to reduce the feature dimensions in the second step. In designing the feature clustering algorithm, a new metaheuristic algorithm is introduced which is called the crocodiles hunting strategy optimization algorithm (CHS) that simulates crocodiles’ behavior in hunting. Also, the feature clustering algorithm introduced the new chromosome encoding for feature clustering which is called feature clustering based on the crocodiles hunting strategy optimization algorithm (FC-CHS). Finally, in the third step, the support vector machine (SVM) algorithm is used for classification. According to the results of classification on the MRI images, the proposed algorithm has achieved high accuracy in Google-Net and ResNet features based on confusion matrices. For comparing the performance of the FC-CHS, this algorithm is compared with five well-known dimension reduction algorithms. Also, real data are used to further investigate the performance of the FC-CHS algorithm. The results show that the combination of the FC-CHS and SVM algorithms have been reached high accuracy in Iris, and Wine data, and in other real data, the proposed algorithm is outperformed compared to other dimension reduction methods in most cases.},
   author = {Alireza Balavand},
   doi = {10.1007/S00371-020-02009-X/FIGURES/22},
   issn = {01782789},
   issue = {1},
   journal = {Visual Computer},
   keywords = {Crocodiles hunting strategy optimization algorithm (CHS),Feature clustering,Google-Net,MRI images,ResNet,Support vector machine},
   month = {1},
   pages = {149-178},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A new feature clustering method based on crocodiles hunting strategy optimization algorithm for classification of MRI images},
   volume = {38},
   url = {https://link.springer.com/article/10.1007/s00371-020-02009-x},
   year = {2022},
}
@article{,
   abstract = {Ensemble learning is a prolific field in Machine Learning since it is based on the assumption that combining the output of multiple models is better than using a single model, and it usually provides good results. Normally, it has been commonly employed for classification, but it can be used to improve other disciplines such as feature selection. Feature selection consists of selecting the relevant features for a problem and discard those irrelevant or redundant, with the main goal of improving classification accuracy. In this work, we provide the reader with the basic concepts necessary to build an ensemble for feature selection, as well as reviewing the up-to-date advances and commenting on the future trends that are still to be faced.},
   author = {Verónica Bolón-Canedo and Amparo Alonso-Betanzos},
   doi = {10.1016/j.inffus.2018.11.008},
   issn = {15662535},
   journal = {Inf. Fusion},
   keywords = {Ensemble learning,Feature selection},
   month = {12},
   pages = {1-12},
   publisher = {Elsevier B.V.},
   title = {Ensembles for feature selection: a review and future trends},
   volume = {52},
   year = {2019},
}
@article{Zhang2020,
   abstract = {Image-based food pattern classification poses new challenges for mainstream computer vision algorithms. Recent works on feature fusion technique have significantly boosted the generalization performances of food categorization tasks. However, the use of representation learning in the training process of feature fusion has rarely been explored. This study addresses the issue through a new supervised subnetwork-based feature encoding and pattern classification model, termed a wide hierarchical subnetwork-based neural network (Wi-HSNN). In particular, Wi-HSNN is a subnet-based iterative training process in which one pair of subnets is added to the framework in each iteration. Furthermore, instead of learning the optimal representations with the whole dataset, this paper introduces a batch-by-batch parallel scheme of Wi-HSNN to process large-scale datasets, such as Place365 set with more than 1.8 million samples. Extensive evaluations on eight benchmark datasets from food classification to scene image recognition demonstrated that the proposed solution has better representation learning capacity compared to existing encoding methods, and achieves stronger performance than existing approaches for food image classification tasks.},
   author = {Wandong Zhang and Jonathan Wu and Yimin Yang},
   doi = {10.1016/J.NEUCOM.2020.07.018},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Deep learning,Food classification,Representation learning,Subnetwork-based multi-layer neural network (SN-MLNN)},
   month = {11},
   pages = {57-66},
   publisher = {Elsevier},
   title = {Wi-HSNN: A subnetwork-based encoding structure for dimension reduction and food classification via harnessing multi-CNN model high-level features},
   volume = {414},
   year = {2020},
}
@article{Schulz2019,
   abstract = {Machine learning algorithms using deep architectures have been able to implement increasingly powerful and successful models. However, they also become increasingly more complex, more difficult to comprehend and easier to fool. So far, most methods in the literature investigate the decision of the model for a single given input datum. In this paper, we propose to visualize a part of the decision function of a deep neural network together with a part of the data set in two dimensions with discriminative dimensionality reduction. This enables us to inspect how different properties of the data are treated by the model, such as outliers, adversaries or poisoned data. Further, the presented approach is complementary to the mentioned interpretation methods from the literature and hence might be even more useful in combination with those. Code is available at https://github.com/LucaHermes/DeepView .},
   author = {Alexander Schulz and Fabian Hinder and Barbara Hammer},
   doi = {10.24963/ijcai.2020/319},
   journal = {IJCAI International Joint Conference on Artificial Intelligence},
   month = {9},
   pages = {2305-2311},
   publisher = {International Joint Conferences on Artificial Intelligence},
   title = {DeepView: Visualizing Classification Boundaries of Deep Neural Networks as Scatter Plots Using Discriminative Dimensionality Reduction},
   volume = {2021-January},
   url = {http://arxiv.org/abs/1909.09154 http://dx.doi.org/10.24963/ijcai.2020/319},
   year = {2019},
}
@article{Peeples2022,
   abstract = {Feature representation is an important aspect of remote-sensing-based image classification. While deep convolutional neural networks (DCNNs) are able to effectively amalgamate information, large numbers of parameters often make learned features inscrutable and difficult to transfer to alternative models. In order to better represent statistical texture information for remote-sensing image classification, in this letter, we investigate performing joint dimensionality reduction (DR) and classification using a novel histogram neural network. Motivated by a popular DR approach, t-distributed stochastic neighbor embedding (t-SNE), our proposed method incorporates a classification loss computed on samples in a low-dimensional embedding space. We compare the learned sample embeddings against coordinates found by t-SNE in terms of classification accuracy and qualitative assessment. We also explore the use of various divergence measures in the t-SNE objective. The proposed method has several advantages such as readily embedding out-of-sample points and reducing feature dimensionality while retaining class discriminability. Our results show that the proposed approach maintains and/or improves classification performance and reveals characteristics of features produced by neural networks that may be helpful for other applications.},
   author = {Joshua Peeples and Sarah Walker and Connor McCurley and Alina Zare and James Keller and Weihuang Xu},
   doi = {10.1109/LGRS.2022.3156532},
   issn = {15580571},
   journal = {IEEE Geoscience and Remote Sensing Letters},
   keywords = {Convolutional neural networks,dimensionality reduction (DR),t-distributed stochastic neighbor embedding (t-SNE)},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Divergence Regulated Encoder Network for Joint Dimensionality Reduction and Classification},
   volume = {19},
   year = {2022},
}
@article{Huang2019,
   abstract = {The power of neural networks lies in their ability to generalize to unseen data, yet the underlying reasons for this phenomenon remain elusive. Numerous rigorous attempts have been made to explain generalization, but available bounds are still quite loose, and analysis does not always lead to true understanding. The goal of this work is to make generalization more intuitive. Using visualization methods, we discuss the mystery of generalization, the geometry of loss landscapes, and how the curse (or, rather, the blessing) of dimensionality causes optimizers to settle into minima that generalize well.},
   author = {W. Ronny Huang and Zeyad Emam and Micah Goldblum and Liam Fowl and J. K. Terry and Furong Huang and Tom Goldstein},
   month = {6},
   title = {Understanding Generalization through Visualizations},
   url = {http://arxiv.org/abs/1906.03291},
   year = {2019},
}
@article{Reddy2020,
   abstract = {Due to digitization, a huge volume of data is being generated across several sectors such as healthcare, production, sales, IoT devices, Web, organizations. Machine learning algorithms are used to uncover patterns among the attributes of this data. Hence, they can be used to make predictions that can be used by medical practitioners and people at managerial level to make executive decisions. Not all the attributes in the datasets generated are important for training the machine learning algorithms. Some attributes might be irrelevant and some might not affect the outcome of the prediction. Ignoring or removing these irrelevant or less important attributes reduces the burden on machine learning algorithms. In this work two of the prominent dimensionality reduction techniques, Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are investigated on four popular Machine Learning (ML) algorithms, Decision Tree Induction, Support Vector Machine (SVM), Naive Bayes Classifier and Random Forest Classifier using publicly available Cardiotocography (CTG) dataset from University of California and Irvine Machine Learning Repository. The experimentation results prove that PCA outperforms LDA in all the measures. Also, the performance of the classifiers, Decision Tree, Random Forest examined is not affected much by using PCA and LDA.To further analyze the performance of PCA and LDA the eperimentation is carried out on Diabetic Retinopathy (DR) and Intrusion Detection System (IDS) datasets. Experimentation results prove that ML algorithms with PCA produce better results when dimensionality of the datasets is high. When dimensionality of datasets is low it is observed that the ML algorithms without dimensionality reduction yields better results.},
   author = {G. Thippa Reddy and M. Praveen Kumar Reddy and Kuruva Lakshmanna and Rajesh Kaluri and Dharmendra Singh Rajput and Gautam Srivastava and Thar Baker},
   doi = {10.1109/ACCESS.2020.2980942},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Cardiotocography dataset,dimensionality reduction,feature engineering,linear discriminant analysis,machine learning,principal component analysis},
   pages = {54776-54788},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Analysis of Dimensionality Reduction Techniques on Big Data},
   volume = {8},
   year = {2020},
}
@article{Zhao2022,
   abstract = {The pooling layer has achieved good results in reducing the feature dimension and parameters of convolution neural network (CNN), but it will cause different degrees of information loss. In order to retain as much feature information as possible, we design a pooling method based on Principal Component Analysis (PCA)-PCAPool. Firstly, all feature maps are traversed with the pooling window in which the data is extracted and stretched into row vectors. With the sliding of the pooling window, all row vectors are arranged in the matrix to form the sample matrix. Then all eigenvectors are extracted from the sample matrix by PCA algorithm to form the eigenvector matrix, which right multiplies the sample matrix to get the principal component matrix. Thirdly, each column of the principal component matrix is weighted with information coefficient which is determined by training to get the pooling vector. Finally, PCAPool result is obtained by blocks arrangement of pooling vector. PCAPool is tested with CNN-Quick, NIN, WRN-SAM, GP-CNN on datasets MNIST, CIFAR10/100 and SVHN. We also used AlexNet on Imagenet2012 to test PCAPool. The experiment results show that compared with traditional pooling methods, PCAPool could retain information in the pooling window better and improve the image classification accuracy.},
   author = {Baiting Zhao and Xiao Dong and Yongcun Guo and Xiaofen Jia and Yourui Huang},
   doi = {10.1007/S11063-021-10632-5/FIGURES/9},
   issn = {1573773X},
   issue = {1},
   journal = {Neural Processing Letters},
   keywords = {CNN,Classification,Dimensionality reduction,PCA,Pooling},
   month = {2},
   pages = {347-368},
   publisher = {Springer},
   title = {PCA Dimensionality Reduction Method for Image Classification},
   volume = {54},
   url = {https://link.springer.com/article/10.1007/s11063-021-10632-5},
   year = {2022},
}
@article{Brown2012,
   abstract = {The world's corpora of data grow in size and complexity every day, making it increasingly difficult for experts to make sense out of their data. Although machine learning offers algorithms for finding patterns in data automatically, they often require algorithm-specific parameters, such as an appropriate distance function, which are outside the purview of a domain expert. We present a system that allows an expert to interact directly with a visual representation of the data to define an appropriate distance function, thus avoiding direct manipulation of obtuse model parameters. Adopting an iterative approach, our system first assumes a uniformly weighted Euclidean distance function and projects the data into a two-dimensional scatterplot view. The user can then move incorrectly-positioned data points to locations that reflect his or her understanding of the similarity of those data points relative to the other data points. Based on this input, the system performs an optimization to learn a new distance function and then re-projects the data to redraw the scatter-plot. We illustrate empirically that with only a few iterations of interaction and optimization, a user can achieve a scatterplot view and its corresponding distance function that reflect the user's knowledge of the data. In addition, we evaluate our system to assess scalability in data size and data dimension, and show that our system is computationally efficient and can provide an interactive or near-interactive user experience. © 2012 IEEE.},
   author = {Eli T. Brown and Jingjing Liu and Carla E. Brodley and Remco Chang},
   doi = {10.1109/VAST.2012.6400486},
   isbn = {9781467347532},
   journal = {IEEE Conference on Visual Analytics Science and Technology 2012, VAST 2012 - Proceedings},
   pages = {83-92},
   title = {Dis-function: Learning distance functions interactively},
   year = {2012},
}
@article{Sacha2017,
   abstract = {Dimensionality Reduction (DR) is a core building block in visualizing multidimensional data. For DR techniques to be useful in exploratory data analysis, they need to be adapted to human needs and domain-specific problems, ideally, interactively, and on-the-fly. Many visual analytics systems have already demonstrated the benefits of tightly integrating DR with interactive visualizations. Nevertheless, a general, structured understanding of this integration is missing. To address this, we systematically studied the visual analytics and visualization literature to investigate how analysts interact with automatic DR techniques. The results reveal seven common interaction scenarios that are amenable to interactive control such as specifying algorithmic constraints, selecting relevant features, or choosing among several DR algorithms. We investigate specific implementations of visual analysis systems integrating DR, and analyze ways that other machine learning methods have been combined with DR. Summarizing the results in a 'human in the loop' process model provides a general lens for the evaluation of visual interactive DR systems. We apply the proposed model to study and classify several systems previously described in the literature, and to derive future research opportunities.},
   author = {Dominik Sacha and Leishi Zhang and Michael Sedlmair and John A. Lee and Jaakko Peltonen and Daniel Weiskopf and Stephen C. North and Daniel A. Keim},
   doi = {10.1109/TVCG.2016.2598495},
   issn = {10772626},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   keywords = {Interactive visualization,dimensionality reduction,machine learning,visual analytics},
   month = {1},
   pages = {241-250},
   pmid = {27875141},
   publisher = {IEEE Computer Society},
   title = {Visual Interaction with Dimensionality Reduction: A Structured Literature Analysis},
   volume = {23},
   year = {2017},
}
@article{Bertini2009,
   abstract = {The aim of this work is to survey and reflect on the various ways to integrate visualization and data mining techniques toward a mixed-initiative knowledge discovery taking the best of human and machine capabilities. Following a bottom-up bibliographic research approach, the article categorizes the observed techniques in classes, highlighting current trends, gaps, and potential future directions for research. In particular it looks at strengths and weaknesses of information visualization and data mining, and for which purposes researchers in infovis use data mining techniques and reversely how researchers in data mining employ infovis techniques. The article further uses this information to analyze the discovery process by comparing the analysis steps from the perspective of information visualization and data mining. The comparison permits to bring to light new perspectives on how mining and visualization can best employ human and machine skills.© 2009 ACM.},
   author = {Enrico Bertini and Denis Lalanne},
   doi = {10.1145/1562849.1562851},
   isbn = {9781605586700},
   journal = {Proceedings of the ACM SIGKDD Workshop on Visual Analytics and Knowledge Discovery, VAKD '09},
   keywords = {Data mining,Knowledge discovery,Visual analytics,Visual data mining,Visualization},
   pages = {12-20},
   title = {Surveying the complementary role of automatic data analysis and visualization in knowledge discovery},
   year = {2009},
}
@article{Card1991,
   abstract = {This paper proposes a concept for the user interface of information retrieval systems called an information workspace. The concept goes beyond the usual notion of an information retrieval system to encompass the cost structure of information from secondary storage to immediate use. As an implementation of the concept, the paper describes an experimental system, called the Information Visualizer, and its rationale. The system is based on (1) the use of 3D/Rooms for increasing the capacity of immediate storage available to the user, (2) the Cognitive Co-processor scheduler-based user interface interaction architecture for coupling the user to information agents, and (3) the use of information visualization for interacting with information structure. © 1991 ACM.},
   author = {Stuart K. Card and George G. Robertson and Jock D. Mackinlay},
   doi = {10.1145/108844.108874},
   isbn = {0897913833},
   journal = {Conference on Human Factors in Computing Systems - Proceedings},
   keywords = {3D graphics,Animation,Desktop metaphor,Information retrieval,Information visualization,Interactive graphics,Interface metaphors,UI theory},
   pages = {181-188},
   publisher = {Association for Computing Machinery},
   title = {The information visualizer, an information workspace},
   year = {1991},
}
@article{Choo2009,
   abstract = {In this paper, we discuss dimension reduction methods for 2D visualization of high dimensional clustered data. We propose a two-stage framework for visualizing such data based on dimension reduction methods. In the first stage, we obtain the reduced dimensional data by applying a supervised dimension reduction method such as linear discriminant analysis which preserves the original cluster structure in terms of its criteria. The resulting optimal reduced dimension depends on the optimization criteria and is often larger than 2. In the second stage, the dimension is further reduced to 2 for visualization purposes by another dimension reduction method such as principal component analysis. The role of the second-stage is to minimize the loss of information due to reducing the dimension all the way to 2. Using this framework, we propose several two-stage methods, and present their theoretical characteristics as well as experimental comparisons on both artificial and real-world text data sets. ©2009 IEEE.},
   author = {Jaegul Choo and Shawn Bohn and Haesun Park},
   doi = {10.1109/VAST.2009.5332629},
   isbn = {9781424452835},
   journal = {VAST 09 - IEEE Symposium on Visual Analytics Science and Technology, Proceedings},
   keywords = {2D projection,Clustered data,Dimension reduction,Generalized singular value decomposition,Linear discriminant analysis,Orthogonal centroid method,Principal component analysis,Regularization},
   pages = {67-74},
   title = {Two-stage framework for visualization of clustered high dimensional data},
   year = {2009},
}
@article{Choo2010,
   abstract = {We present an interactive visual analytics system for classification, iVisClassifier, based on a supervised dimension reduction method, linear discriminant analysis (LDA). Given high-dimensional data and associated cluster labels, LDA gives their reduced dimensional representation, which provides a good overview about the cluster structure. Instead of a single two- or three-dimensional scatter plot, iVisClassifier fully interacts with all the reduced dimensions obtained by LDA through parallel coordinates and a scatter plot. Furthermore, it significantly improves the interactivity and interpretability of LDA. LDA enables users to understand each of the reduced dimensions and how they influence the data by reconstructing the basis vector into the original data domain. By using heat maps, iVisClassifier gives an overview about the cluster relationship in terms of pairwise distances between cluster centroids both in the original space and in the reduced dimensional space. Equipped with these functionalities, iVisClassifier supports users' classification tasks in an efficient way. Using several facial image data, we show how the above analysis is performed. ©2010 IEEE.},
   author = {Jaegul Choo and Hanseung Lee and Jaeyeon Kihm and Haesun Park},
   doi = {10.1109/VAST.2010.5652443},
   isbn = {9781424494866},
   journal = {VAST 10 - IEEE Conference on Visual Analytics Science and Technology 2010, Proceedings},
   keywords = {H.5.2 [information interfaces and presentation]: user interfaces - theory and methods},
   pages = {27-34},
   title = {iVisClassifier: An interactive visual analytics system for classification based on supervised dimension reduction},
   year = {2010},
}
@article{Endert2014,
   abstract = {Visual analytics is the science of marrying interactive visualizations and analytic algorithms to support exploratory knowledge discovery in large datasets. We argue for a shift from a ‘human in the loop’ philosophy for visual analytics to a ‘human is the loop’ viewpoint, where the focus is on recognizing analysts’ work processes, and seamlessly fitting analytics into that existing interactive process. We survey a range of projects that provide visual analytic support contextually in the sensemaking loop, and outline a research agenda along with future challenges.},
   author = {Alex Endert and M. Shahriar Hossain and Naren Ramakrishnan and Chris North and Patrick Fiaux and Christopher Andrews},
   doi = {10.1007/S10844-014-0304-9},
   issn = {15737675},
   issue = {3},
   journal = {Journal of Intelligent Information Systems},
   keywords = {Clustering,Semantic interaction,Spatialization,Storytelling,Visual analytics},
   month = {12},
   pages = {411-435},
   publisher = {Kluwer Academic Publishers},
   title = {The human is the loop: new directions for visual analytics},
   volume = {43},
   year = {2014},
}
@article{Garg2010,
   abstract = {The process of learning models from raw data typically requires a substantial amount of user input during the model initialization phase. We present an assistive visualization system which greatly reduces the load on the users and makes the process of model initialization and refinement more efficient, problem-driven, and engaging. Utilizing a sequence segmentation task with a Hidden Markov Model as an example, we assign each token in the sequence a feature vector based on its various properties within the sequence. These vectors are then clustered according to similarity, generating a layout of the individual tokens in form of a node link diagram where the length of the links is determined by the feature vector similarity. Users may then tune the weights of the feature vector components to improve the segmentation, which is visualized as a better separation of the clusters. Also, as individual clusters represent different classes, the user can now work at the cluster level to define token classes, instead of labelling one entry at time. Inconsistent entries visually identify themselves by locating at the periphery of clusters, and the user then helps refine the model by resolving these inconsistencies. Our system therefore makes efficient use of the knowledge of its users, only requesting user assistance for non-trivial data items. It so allows users to visually analyse data at a higher, more abstract level, improving scalability. ©2010 IEEE.},
   author = {Supriya Garg and I. V. Ramakrishnan and Klaus Mueller},
   doi = {10.1109/VAST.2010.5652484},
   isbn = {9781424494866},
   journal = {VAST 10 - IEEE Conference on Visual Analytics Science and Technology 2010, Proceedings},
   keywords = {Data clustering,Human-Computer interaction,Visual knowledge discovery,Visual knowledge representation},
   pages = {67-74},
   title = {A visual analytics approach to model learning},
   year = {2010},
}
@article{,
   abstract = {A comprehensive introduction to ICA for students and practitioners Independent Component Analysis (ICA) is one of the most exciting new topics in fields such as neural networks, advanced statistics, and signal processing. This is the first book to provide a comprehensive introduction to this new technique complete with the fundamental mathematical background needed to understand and utilize it. It offers a general overview of the basics of ICA, important solutions and algorithms, and in-depth coverage of new applications in image processing, telecommunications, audio signal processing, and more. Independent Component Analysis is divided into four sections that cover: General mathematical concepts utilized in the book The basic ICA model and its solution Various extensions of the basic ICA model Real-world applications for ICA models Authors Hyvarinen, Karhunen, and Oja are well known for their contributions to the development of ICA and here cover all the relevant theory, new algorithms, and applications in various fields. Researchers, students, and practitioners from a variety of disciplines will find this accessible volume both helpful and informative.},
   author = {Aapo Hyvärinen and Juha Karhunen and Erkki Oja},
   doi = {10.1002/0471221317},
   journal = {Independent Component Analysis},
   month = {5},
   publisher = {John Wiley & Sons, Inc.},
   title = {Independent Component Analysis},
   year = {2001},
}
@article{Jeong2009,
   abstract = {Principle Component Analysis (PCA) is a widely used mathematical technique in many fields for factor and trend analysis, dimension reduction, etc. However, it is often considered to be a "black box" operation whose results are difficult to interpret and sometimes counter-intuitive to the user. In order to assist the user in better understanding and utilizing PCA, we have developed a system that visualizes the results of principal component analysis using multiple coordinated views and a rich set of user interactions. Our design philosophy is to support analysis of multivariate datasets through extensive interaction with the PCA output. To demonstrate the usefulness of our system, we performed a comparative user study with a known commercial system, SAS/INSIGHT's Interactive Data Exploration. Participants in our study solved a number of high-level analysis tasks with each interface and rated the systems on ease of learning and usefulness. Based on the participants' accuracy, speed, and qualitative feedback, we observe that our system helps users to better understand relationships between the data and the calculated eigenspace, which allows the participants to more accurately analyze the data. User feedback suggests that the interactivity and transparency of our system are the key strengths of our approach. © 2009 The Eurographics Association and Blackwell Publishing Ltd.},
   author = {Dong Hyun Jeong and Caroline Ziemkiewicz and Brian Fisher and William Ribarsky and Remco Chang},
   doi = {10.1111/J.1467-8659.2009.01475.X},
   issn = {14678659},
   issue = {3},
   journal = {Computer Graphics Forum},
   pages = {767-774},
   publisher = {Blackwell Publishing Ltd},
   title = {IPCA: An interactive system for PCA-based visual analytics},
   volume = {28},
   year = {2009},
}
@article{Joia2015,
   abstract = {Multidimensional projection-based visualization methods typically rely on clustering and attribute selection mechanisms to enable visual analysis of multidimensional data. Clustering is often employed to group similar instances according to their distance in the visual space. However, considering only distances in the visual space may be misleading due to projection errors as well as the lack of guarantees to ensure that distinct clusters contain instances with different content. Identifying clusters made up of a few elements is also an issue for most clustering methods. In this work we propose a novel multidimensional projection-based visualization technique that relies on representative instances to define clusters in the visual space. Representative instances are selected by a deterministic sampling scheme derived from matrix decomposition, which is sensitive to the variability of data while still been able to handle classes with a small number of instances. Moreover, the sampling mechanism can easily be adapted to select relevant attributes from each cluster. Therefore, our methodology unifies sampling, clustering, and feature selection in a simple framework. A comprehensive set of experiments validate our methodology, showing it outperforms most existing sampling and feature selection techniques. A case study shows the effectiveness of the proposed methodology as a visual data analysis tool.},
   author = {P. Joia and F. Petronetto and L. G. Nonato},
   doi = {10.1111/CGF.12640},
   issn = {14678659},
   issue = {3},
   journal = {Computer Graphics Forum},
   month = {6},
   pages = {281-290},
   publisher = {Blackwell Publishing Ltd},
   title = {Uncovering Representative Groups in Multidimensional Projections},
   volume = {34},
   year = {2015},
}
@article{,
   abstract = {Methods of dimensionality reduction provide a way to understand and visualize the structure of complex data sets. Traditional methods like principal component analysis and classical metric multidimensional scaling suffer from being based on linear models. Until recently, very few methods were able to reduce the data dimensionality in a nonlinear way. However, since the late nineties, many new methods have been developed and nonlinear dimensionality reduction, also called manifold learning, has become a hot topic. New advances that account for this rapid growth are, e.g. the use of graphs to represent the manifold topology, and the use of new metrics like the geodesic distance. In addition, new optimization schemes, based on kernel techniques and spectral decomposition, have lead to spectral embedding, which encompasses many of the recently developed methods. This book describes existing and advanced methods to reduce the dimensionality of numerical databases. For each method, the description starts from intuitive ideas, develops the necessary mathematical details, and ends by outlining the algorithmic implementation. Methods are compared with each other with the help of different illustrative examples. The purpose of the book is to summarize clear facts and ideas about well-known methods as well as recent developments in the topic of nonlinear dimensionality reduction. With this goal in mind, methods are all described from a unifying point of view, in order to highlight their respective strengths and shortcomings. The book is primarily intended for statisticians, computer scientists and data analysts. It is also accessible to other practitioners having a basic background in statistics and/or computational learning, like psychologists (in psychometry) and economists. John A. Lee is a Postdoctoral Researcher of the Belgian National Fund for Scientific Research (FNRS). He is (co-)author of more than 30 publications in the field of machine learning and dimensionality reduction. Michel Verleysen is Professor at the Universit catholique de Louvain (Louvain-la-Neuve, Belgium), and Honorary Research Director of the Belgian National Fund for Scientific Research (FNRS). He is the chairman of the annual European Symposium on Artificial Neural Networks, co-editor of the Neural Processing Letters journal (Springer), and (co-)author of more than 200 scientific publications in the field of machine learning.},
   doi = {10.1007/978-0-387-39351-3},
   journal = {Nonlinear Dimensionality Reduction},
   publisher = {Springer New York},
   title = {Nonlinear Dimensionality Reduction},
   year = {2007},
}
@article{Liu2014,
   abstract = {Dimension reduction techniques are essential for feature selection and feature extraction of complex high-dimensional data. These techniques, which construct low-dimensional representations of data, are typically geometrically motivated, computationally efficient and approximately preserve certain structural properties of the data. However, they are often used as black box solutions in data exploration and their results can be difficult to interpret. To assess the quality of these results, quality measures, such as co-ranking [LV09], have been proposed to quantify structural distortions that occur between high-dimensional and low-dimensional data representations. Such measures could be evaluated and visualized point-wise to further highlight erroneous regions [MLGH13]. In this work, we provide an interactive visualization framework for exploring high-dimensional data via its two-dimensional embeddings obtained from dimension reduction, using a rich set of user interactions. We ask the following question: what new insights do we obtain regarding the structure of the data, with interactive manipulations of its embeddings in the visual space? We augment the two-dimensional embeddings with structural abstractions obtained from hierarchical clusterings, to help users navigate and manipulate subsets of the data. We use point-wise distortion measures to highlight interesting regions in the domain, and further to guide our selection of the appropriate level of clusterings that are aligned with the regions of interest. Under the static setting, point-wise distortions indicate the level of structural uncertainty within the embeddings. Under the dynamic setting, on-the-fly updates of point-wise distortions due to data movement and data deletion reflect structural relations among different parts of the data, which may lead to new and valuable insights. © 2014 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.},
   author = {S. Liu and B. Wang and P. T. Bremer and V. Pascucci},
   doi = {10.1111/CGF.12366},
   issn = {14678659},
   issue = {3},
   journal = {Computer Graphics Forum},
   pages = {101-110},
   publisher = {Blackwell Publishing Ltd},
   title = {Distortion-guided structure-driven interactive exploration of high-dimensional data},
   volume = {33},
   year = {2014},
}
@article{Liu2015,
   abstract = {We introduce a novel interactive framework for visualizing and exploring high-dimensional datasets based on subspace analysis and dynamic projections. We assume the high-dimensional dataset can be represented by a mixture of low-dimensional linear subspaces with mixed dimensions, and provide a method to reliably estimate the intrinsic dimension and linear basis of each subspace extracted from the subspace clustering. Subsequently, we use these bases to define unique 2D linear projections as viewpoints from which to visualize the data. To understand the relationships among the different projections and to discover hidden patterns, we connect these projections through dynamic projections that create smooth animated transitions between pairs of projections. We introduce the view transition graph, which provides flexible navigation among these projections to facilitate an intuitive exploration. Finally, we provide detailed comparisons with related systems, and use real-world examples to demonstrate the novelty and usability of our proposed framework.},
   author = {S. Liu and B. Wang and J. J. Thiagarajan and P. T. Bremer and V. Pascucci},
   doi = {10.1111/CGF.12639},
   issn = {14678659},
   issue = {3},
   journal = {Computer Graphics Forum},
   keywords = {I.3.3 [Computer Graphics]: Picture/Image Generation - Line and curve generation},
   month = {6},
   pages = {271-280},
   publisher = {Blackwell Publishing Ltd},
   title = {Visual Exploration of High-Dimensional Data through Subspace Analysis and Dynamic Projections},
   volume = {34},
   year = {2015},
}
@article{Rieck2015,
   abstract = {High-dimensional data sets are a prevalent occurrence in many application domains. This data is commonly visualized using dimensionality reduction (DR) methods. DR methods provide e.g. a two-dimensional embedding of the abstract data that retains relevant high-dimensional characteristics such as local distances between data points. Since the amount of DR algorithms from which users may choose is steadily increasing, assessing their quality becomes more and more important. We present a novel technique to quantify and compare the quality of DR algorithms that is based on persistent homology. An inherent beneficial property of persistent homology is its robustness against noise which makes it well suited for real world data. Our pipeline informs about the best DR technique for a given data set and chosen metric (e.g. preservation of local distances) and provides knowledge about the local quality of an embedding, thereby helping users understand the shortcomings of the selected DR method. The utility of our method is demonstrated using application data from multiple domains and a variety of commonly used DR methods.},
   author = {B. Rieck and H. Leitte},
   doi = {10.1111/CGF.12655},
   issn = {14678659},
   issue = {3},
   journal = {Computer Graphics Forum},
   keywords = {I.3.6 [Computer Graphics]: Methodology and Techniques - Interaction techniques},
   month = {6},
   pages = {431-440},
   publisher = {Blackwell Publishing Ltd},
   title = {Persistent Homology for the Evaluation of Dimensionality Reduction Schemes},
   volume = {34},
   year = {2015},
}
@article{,
   abstract = {In various application areas (social science, transportation, or medicine) analysts need to gain knowledge from large amounts of data. This analysis is often supported by interactive Visual Analytics tools that combine automatic analysis with interactive visualization. Such a data analysis process is not streamlined, but consists of several steps and feedback loops. In order to be able to optimize the process, identify problems, or common problem solving strategies, recording and reproducibility of this process is needed. This is facilitated by tracking of user actions categorized according to a taxonomy of interactions. Visual Analytics includes several means of interaction that are differentiated according to three fields: Information visualization, reasoning, and data processing. At present, however, only separate taxonomies for interaction techniques exist in these three fields. Each taxonomy covers only a part of the actions undertaken in Visual Analytics. Moreover, as they use different foundations (user intentions vs. User actions) and employ different terminology, it is not clear to what extent they overlap and cover the whole Visual Analytics interaction space. We therefore first compare them and then elaborate a new integrated taxonomy in the context of Visual Analytics. In order to show the usability of the new taxonomy, we specify it on visual graph analysis and apply it to the tracking of user interactions in this area.},
   author = {Tatiana von Landesberger and Sebastian Fiebig and Sebastian Bremm and Arjan Kuijper and Dieter W. Fellner},
   doi = {10.1007/978-1-4614-7485-2_26},
   isbn = {9781461474852},
   journal = {Handbook of Human Centric Visualization},
   month = {1},
   pages = {653-670},
   publisher = {Springer New York},
   title = {Interaction taxonomy for tracking of user actions in visual analytics applications},
   year = {2014},
}
@article{Mamani2013,
   abstract = {Interactive visualization systems for exploring and manipulating high-dimensional feature spaces have experienced a substantial progress in the last few years. State-of-art methods rely on solid mathematical and computational foundations that enable sophisticated and flexible interactive tools. Current methods are even capable of modifying data attributes during interaction, highlighting regions of potential interest in the feature space, and building visualizations that bring out the relevance of attributes. However, those methodologies rely on complex and non-intuitive interfaces that hamper the free handling of the feature spaces. Moreover, visualizing how neighborhood structures are affected during the space manipulation is also an issue for existing methods. This paper presents a novel visualization-assisted methodology for interacting and transforming data attributes embedded in feature spaces. The proposed approach relies on a combination of multidimensional projections and local transformations to provide an interactive mechanism for modifying attributes. Besides enabling a simple and intuitive visual layout, our approach allows the user to easily observe the changes in neighborhood structures during interaction. The usefulness of our methodology is shown in an application geared to image retrieval. © 2013 The Author(s) Computer Graphics Forum © 2013 The Eurographics Association and Blackwell Publishing Ltd.},
   author = {G. M.H. Mamani and F. M. Fatore and L. G. Nonato and F. V. Paulovich},
   doi = {10.1111/CGF.12116},
   issn = {14678659},
   issue = {3 PART3},
   journal = {Computer Graphics Forum},
   pages = {291-299},
   publisher = {Blackwell Publishing Ltd},
   title = {User-driven feature space transformation},
   volume = {32},
   year = {2013},
}
@article{Hasan2021,
   abstract = {Big databases are increasingly widespread and are therefore hard to understand, in exploratory biomedicine science, big data in health research is highly exciting because data-based analyses can travel quicker than hypothesis-based research. Principal Component Analysis (PCA) is a method to reduce the dimensionality of certain datasets. Improves interpretability but without losing much information. It achieves this by creating new covariates that are not related to each other. Finding those new variables, or what we call the main components, will reduce the eigenvalue /eigenvectors solution problem. (PCA) can be said to be an adaptive data analysis technology because technology variables are developed to adapt to different data types and structures. This review will start by introducing the basic ideas of (PCA), describe some concepts related to (PCA), and discussing. What it can do, and reviewed fifteen articles of (PCA) that have been introduced and published in the last three years.},
   author = {Basna Mohammed Salih Hasan and Adnan Mohsin Abdulazeez},
   issn = {2716-621X},
   issue = {1},
   journal = {Journal of Soft Computing and Data Mining},
   keywords = {dimensionality reduction,principal component analysis},
   month = {4},
   pages = {20-30},
   title = {A Review of Principal Component Analysis Algorithm for Dimensionality Reduction},
   volume = {2},
   url = {https://publisher.uthm.edu.my/ojs/index.php/jscdm/article/view/8032},
   year = {2021},
}
@article{Ivosev2008,
   abstract = {Many modern applications of analytical chemistry involve the collection of large megavariate data sets and subsequent processing with multivariate analysis techniques (MVA), two of the more common goals being data analysis (also known as data mining and exploratory data analysis) and classification. Classification attempts to determine variables that can distinguish known classes allowing unknown samples to be correctly assigned, whereas data analysis seeks to uncover and understand or confirm relationships between the samples and the variables. An important part of analysis is visualization which allows analysts to apply their expertise and knowledge and is often easier for the samples than the variables since there are frequently far more of the latter. Here we describe principal component variable grouping (PCVG), an unsupervised, intuitive method that assigns a large number of variables to a smaller number of groups that can be more readily visualized and understood. Knowledge of the source or nature of the variables in a group allows them all to be appropriately treated, for example, removed if they result from uninteresting effects or replaced by a single representative for further processing. © 2008 American Chemical Society.},
   author = {Gordana Ivosev and Lyle Burton and Ron Bonner},
   doi = {10.1021/AC800110W/ASSET/IMAGES/LARGE/AC-2008-00110W_0008.JPEG},
   issn = {00032700},
   issue = {13},
   journal = {Analytical Chemistry},
   month = {7},
   pages = {4933-4944},
   pmid = {18537272},
   publisher = { American Chemical Society},
   title = {Dimensionality reduction and visualization in principal component analysis},
   volume = {80},
   url = {https://pubs.acs.org/doi/full/10.1021/ac800110w},
   year = {2008},
}
@article{Hasan2021,
   abstract = {Big databases are increasingly widespread and are therefore hard to understand, in exploratory biomedicine science, big data in health research is highly exciting because data-based analyses can travel quicker than hypothesis-based research. Principal Component Analysis (PCA) is a method to reduce the dimensionality of certain datasets. Improves interpretability but without losing much information. It achieves this by creating new covariates that are not related to each other. Finding those new variables, or what we call the main components, will reduce the eigenvalue /eigenvectors solution problem. (PCA) can be said to be an adaptive data analysis technology because technology variables are developed to adapt to different data types and structures. This review will start by introducing the basic ideas of (PCA), describe some concepts related to (PCA), and discussing. What it can do, and reviewed fifteen articles of (PCA) that have been introduced and published in the last three years.},
   author = {Basna Mohammed Salih Hasan and Adnan Mohsin Abdulazeez},
   issn = {2716-621X},
   issue = {1},
   journal = {Journal of Soft Computing and Data Mining},
   keywords = {dimensionality reduction,principal component analysis},
   month = {4},
   pages = {20-30},
   title = {A Review of Principal Component Analysis Algorithm for Dimensionality Reduction},
   volume = {2},
   url = {https://publisher.uthm.edu.my/ojs/index.php/jscdm/article/view/8032},
   year = {2021},
}
@article{Fujiwara2022,
   abstract = {Finding the similarities and differences between groups of datasets is a fundamental analysis task. For high-dimensional data, dimensionality reduction (DR) methods are often used to find the characteristics of each group. However, existing DR methods provide limited capability and flexibility for such comparative analysis as each method is designed only for a narrow analysis target, such as identifying factors that most differentiate groups. This paper presents an interactive DR framework where we integrate our new DR method, called ULCA (unified linear comparative analysis), with an interactive visual interface. ULCA unifies two DR schemes, discriminant analysis and contrastive learning, to support various comparative analysis tasks. To provide flexibility for comparative analysis, we develop an optimization algorithm that enables analysts to interactively refine ULCA results. Additionally, the interactive visualization interface facilitates interpretation and refinement of the ULCA results. We evaluate ULCA and the optimization algorithm to show their efficiency as well as present multiple case studies using real-world datasets to demonstrate the usefulness of this framework.},
   author = {Takanori Fujiwara and Xinhai Wei and Jian Zhao and Kwan Liu Ma},
   doi = {10.1109/TVCG.2021.3114807},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   keywords = {Comparative analysis,Contrastive learning,Dimensionality reduction,Discriminant analysis,Interpretability,Visual analytics},
   month = {1},
   pages = {758-768},
   pmid = {34591765},
   publisher = {IEEE Computer Society},
   title = {Interactive Dimensionality Reduction for Comparative Analysis},
   volume = {28},
   year = {2022},
}
@article{Abid2018,
   abstract = {Visualization and exploration of high-dimensional data is a ubiquitous challenge across disciplines. Widely used techniques such as principal component analysis (PCA) aim to identify dominant trends in one dataset. However, in many settings we have datasets collected under different conditions, e.g., a treatment and a control experiment, and we are interested in visualizing and exploring patterns that are specific to one dataset. This paper proposes a method, contrastive principal component analysis (cPCA), which identifies low-dimensional structures that are enriched in a dataset relative to comparison data. In a wide variety of experiments, we demonstrate that cPCA with a background dataset enables us to visualize dataset-specific patterns missed by PCA and other standard methods. We further provide a geometric interpretation of cPCA and strong mathematical guarantees. An implementation of cPCA is publicly available, and can be used for exploratory data analysis in many applications where PCA is currently used.},
   author = {Abubakar Abid and Martin J. Zhang and Vivek K. Bagaria and James Zou},
   doi = {10.1038/S41467-018-04608-8},
   issn = {20411723},
   issue = {1},
   journal = {Nature Communications},
   month = {12},
   pmid = {29849030},
   publisher = {Nature Publishing Group},
   title = {Exploring patterns enriched in a dataset with contrastive principal component analysis},
   volume = {9},
   year = {2018},
}
@article{Boileau2020,
   abstract = {Motivation: Statistical analyses of high-throughput sequencing data have re-shaped the biological sciences. In spite of myriad advances, recovering interpretable biological signal from data corrupted by technical noise remains a prevalent open problem. Several classes of procedures, among them classical dimensionality reduction techniques and others incorporating subject-matter knowledge, have provided effective advances. However, no procedure currently satisfies the dual objectives of recovering stable and relevant features simultaneously. Results: Inspired by recent proposals for making use of control data in the removal of unwanted variation, we propose a variant of principal component analysis (PCA), sparse contrastive PCA that extracts sparse, stable, interpretable and relevant biological signal. The new methodology is compared to competing dimensionality reduction approaches through a simulation study and via analyses of several publicly available protein expression, microarray gene expression and single-cell transcriptome sequencing datasets. Contact: philippe_boileau@berkeley.edu},
   author = {Philippe Boileau and Nima S. Hejazi and Sandrine Dudoit},
   doi = {10.1093/BIOINFORMATICS/BTAA176},
   issn = {14602059},
   issue = {11},
   journal = {Bioinformatics},
   month = {6},
   pages = {3422-3430},
   pmid = {32176249},
   publisher = {Oxford University Press},
   title = {Exploring high-dimensional biological data with sparse contrastive principal component analysis},
   volume = {36},
   year = {2020},
}
@article{Brehmer2014,
   abstract = {We characterize five task sequences related to visualizing dimensionally-reduced data, drawing from data collected from interviews with ten data analysts spanning six application domains, and from our understanding of the technique literature. Our characterization of visualization task sequences for dimensionallyreduced data fills a gap created by the abundance of proposed techniques and tools that combine high-dimensional data analysis, dimensionality reduction, and visualization, and is intended to be used in the design and evaluation of future techniques and tools. We discuss implications for the evaluation of existing work practices, for the design of controlled experiments, and for the analysis of post-deployment field observations.},
   author = {Matthew Brehmer and Michael Sedlmair and Stephen Ingram and Tamara Munzner},
   doi = {10.1145/2669557.2669559},
   isbn = {9781450332095},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Dimensionally-reduced data,Interview study,Tasks},
   month = {11},
   pages = {1-8},
   publisher = {Association for Computing Machinery},
   title = {Visualizing dimensionally-reduced data: Interviews with analysts and a characterization of task sequences},
   volume = {10-November-2015},
   year = {2014},
}
@article{Gleicher2011,
   abstract = {Data analysis often involves the comparison of complex objects. With the ever increasing amounts and complexity of data, the demand for systems to help with these comparisons is also growing. Increasingly, information visualization tools support such comparisons explicitly, beyond simply allowing a viewer to examine each object individually. In this paper, we argue that the design of information visualizations of complex objects can, and should, be studied in general, that is independently of what those objects are. As a first step in developing this general understanding of comparison, we propose a general taxonomy of visual designs for comparison that groups designs into three basic categories, which can be combined. To clarify the taxonomy and validate its completeness, we provide a survey of work in information visualization related to comparison. Although we find a great diversity of systems and approaches, we see that all designs are assembled from the building blocks of juxtaposition, superposition and explicit encodings. This initial exploration shows the power of our model, and suggests future challenges in developing a general understanding of comparative visualization and facilitating the development of more comparative visualization tools. © The Author(s) 2011.},
   author = {Michael Gleicher and Danielle Albers and Rick Walker and Ilir Jusufi and Charles D. Hansen and Jonathan C. Roberts},
   doi = {10.1177/1473871611416549},
   issn = {14738716},
   issue = {4},
   journal = {Information Visualization},
   keywords = {Comparison,Survey,Taxonomy},
   month = {10},
   pages = {289-309},
   title = {Visual comparison for information visualization},
   volume = {10},
   year = {2011},
}
@article{Coimbra2016,
   abstract = {Understanding three-dimensional projections created by dimensionality reduction from high-variate datasets is very challenging. In particular, classical three-dimensional scatterplots used to display such projections do not explicitly show the relations between the projected points, the viewpoint used to visualize the projection, and the original data variables. To explore and explain such relations, we propose a set of interactive visualization techniques. First, we adapt and enhance biplots to show the data variables in the projected threedimensional space. Next, we use a set of interactive bar chart legends to show variables that are visible from a given viewpoint and also assist users to select an optimal viewpoint to examine a desired set of variables. Finally, we propose an interactive viewpoint legend that provides an overview of the information visible in a given three-dimensional projection from all possible viewpoints. Our techniques are simple to implement and can be applied to any dimensionality reduction technique. We demonstrate our techniques on the exploration of several real-world high-dimensional datasets.},
   author = {Danilo B. Coimbra and Rafael M. Martins and Tácito T.A.T. Neves and Alexandru C. Telea and Fernando V. Paulovich},
   doi = {10.1177/1473871615600010},
   issn = {14738724},
   issue = {2},
   journal = {Information Visualization},
   keywords = {Dimensionality reduction,Explanatory visualization,Multivariate visualization,Scatterplots},
   month = {4},
   pages = {154-172},
   publisher = {SAGE Publications Ltd},
   title = {Explaining three-dimensional dimensionality reduction plots},
   volume = {15},
   year = {2016},
}
@article{Fujiwara2020,
   abstract = {Dimensionality reduction (DR) is frequently used for analyzing and visualizing high-dimensional data as it provides a good first glance of the data. However, to interpret the DR result for gaining useful insights from the data, it would take additional analysis effort such as identifying clusters and understanding their characteristics. While there are many automatic methods (e.g., density-based clustering methods) to identify clusters, effective methods for understanding a cluster's characteristics are still lacking. A cluster can be mostly characterized by its distribution of feature values. Reviewing the original feature values is not a straightforward task when the number of features is large. To address this challenge, we present a visual analytics method that effectively highlights the essential features of a cluster in a DR result. To extract the essential features, we introduce an enhanced usage of contrastive principal component analysis (cPCA). Our method, called ccPCA (contrasting clusters in PCA), can calculate each feature's relative contribution to the contrast between one cluster and other clusters. With ccPCA, we have created an interactive system including a scalable visualization of clusters' feature contributions. We demonstrate the effectiveness of our method and system with case studies using several publicly available datasets.},
   author = {Takanori Fujiwara and Oh Hyun Kwon and Kwan Liu Ma},
   doi = {10.1109/TVCG.2019.2934251},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   keywords = {Dimensionality reduction,contrastive learning,high-dimensional data,principal component analysis,visual analytics},
   month = {1},
   pages = {45-55},
   pmid = {31425080},
   publisher = {IEEE Computer Society},
   title = {Supporting Analysis of Dimensionality Reduction Results with Contrastive Learning},
   volume = {26},
   year = {2020},
}
@article{Fujiwara2022,
   abstract = {Finding the similarities and differences between groups of datasets is a fundamental analysis task. For high-dimensional data, dimensionality reduction (DR) methods are often used to find the characteristics of each group. However, existing DR methods provide limited capability and flexibility for such comparative analysis as each method is designed only for a narrow analysis target, such as identifying factors that most differentiate groups. This paper presents an interactive DR framework where we integrate our new DR method, called ULCA (unified linear comparative analysis), with an interactive visual interface. ULCA unifies two DR schemes, discriminant analysis and contrastive learning, to support various comparative analysis tasks. To provide flexibility for comparative analysis, we develop an optimization algorithm that enables analysts to interactively refine ULCA results. Additionally, the interactive visualization interface facilitates interpretation and refinement of the ULCA results. We evaluate ULCA and the optimization algorithm to show their efficiency as well as present multiple case studies using real-world datasets to demonstrate the usefulness of this framework.},
   author = {Takanori Fujiwara and Xinhai Wei and Jian Zhao and Kwan Liu Ma},
   doi = {10.1109/TVCG.2021.3114807},
   issn = {19410506},
   issue = {1},
   journal = {IEEE Transactions on Visualization and Computer Graphics},
   keywords = {Comparative analysis,Contrastive learning,Dimensionality reduction,Discriminant analysis,Interpretability,Visual analytics},
   month = {1},
   pages = {758-768},
   pmid = {34591765},
   publisher = {IEEE Computer Society},
   title = {Interactive Dimensionality Reduction for Comparative Analysis},
   volume = {28},
   year = {2022},
}
@article{Han2022,
   abstract = {Evaluation metrics in image synthesis play a key role to measure performances
of generative models. However, most metrics mainly focus on image fidelity.
Existing diversity metrics are derived by comparing distributions, and thus
they cannot quantify the diversity or rarity degree of each generated image. In
this work, we propose a new evaluation metric, called `rarity score', to
measure the individual rarity of each image synthesized by generative models.
We first show empirical observation that common samples are close to each other
and rare samples are far from each other in nearest-neighbor distances of
feature space. We then use our metric to demonstrate that the extent to which
different generative models produce rare images can be effectively compared. We
also propose a method to compare rarities between datasets that share the same
concept such as CelebA-HQ and FFHQ. Finally, we analyze the use of metrics in
different designs of feature spaces to better understand the relationship
between feature spaces and resulting sparse images. Code will be publicly
available online for the research community.},
   author = {Jiyeon Han and Hwanil Choi and Yunjey Choi and Junho Kim and Jung-Woo Ha and Jaesik Choi},
   month = {6},
   title = {Rarity Score : A New Metric to Evaluate the Uncommonness of Synthesized Images},
   url = {https://arxiv.org/abs/2206.08549v2},
   year = {2022},
}
@article{,
   abstract = {The sensitivity of parameters in computational science problems is difficult
to assess, especially for algorithms with multiple input parameters and diverse
outputs. This work seeks to explore sensitivity analysis in the visualization
domain, introducing novel techniques for respective visual analyses of
parameter sensitivity in multi-dimensional algorithms. First, the sensitivity
analysis background is revisited, highlighting the definition of sensitivity
analysis and approaches analyzing global and local sensitivity as well as the
differences of sensitivity analysis to the more common uncertainty analysis. We
introduce and explore parameter sensitivity using visualization techniques from
overviews to details on demand, covering the analysis of all aspects of
sensitivity in a prototypical implementation. The respective visualization
techniques outline the algorithmic in- and outputs including indications, on
how sensitive an input is with regard to the outputs. The detailed sensitivity
information is communicated through constellation plots for the exploration of
input and output spaces. A matrix view is discussed for localized information
on the sensitivity of specific outputs to specific inputs. A 3D view provides
the link of the parameter sensitivity to the spatial domain, in which the
results of the multi-dimensional algorithms are embedded. The proposed
sensitivity analysis techniques are implemented and evaluated in a prototype
called Sensitivity Explorer. We show that Sensitivity Explorer reliably
identifies the most influential parameters and provides insights into which of
the output characteristics these affect as well as to which extent.},
   author = {Bernhard Fröhler and Tim Elberfeld and Torsten Möller and Hans-Christian Hege and Julia Maurer and Christoph Heinzl},
   month = {4},
   title = {Sensitive vPSA -- Exploring Sensitivity in Visual Parameter Space Analysis},
   url = {https://arxiv.org/abs/2204.01823v1},
   year = {2022},
}
@article{Schmitt2022,
   abstract = {Deep Learning (DL) is leveraged in a growing number of industrial applications. One strength is the data-driven ability to extract characteristic features from complex inputs in form of a latent vector without the need for closed formulation or derivation from a priori known quantities. This work proposes a framework based on generative DL methods to interpret these latent vectors as metrological quantities. The approach is explored in the machine vision domain by implementing a model utilising style-based adversarial latent autoencoders, principal component analysis, and logistic regression. It is successfully evaluated on an industrial image set of aluminium die casting surfaces.},
   author = {Robert H. Schmitt and Dominik Wolfschläger and Evelina Masliankova and Benjamin Montavon},
   doi = {10.1016/J.CIRP.2022.03.016},
   issn = {0007-8506},
   issue = {1},
   journal = {CIRP Annals},
   keywords = {Artificial Intelligence,In-process measurement,Metrology,Surface analysis},
   month = {1},
   pages = {433-436},
   publisher = {Elsevier},
   title = {Metrologically interpretable feature extraction for industrial machine vision using generative deep learning},
   volume = {71},
   year = {2022},
}
@article{Zhu2022,
   abstract = {In current industrial production, the acquisition and processing of large amounts of sensor monitoring data have become an important component of equipment monitoring. Artificial intelligence approaches have the advantage in the management ability of big data from manufacturing processes and are thus increasingly used in industry. However, the artificial intelligence methods are suitable for specific production lines and hardly generalizable to new situations. To solve these problems, this paper proposes a transfer learning-based Stacked Auto-encoder (SAE) with Convolutional Neural Network (CNN), which simplifies high-dimensional data into generalization and effective features that characterize the machine tool's working state. First, it uses an unsupervised SAE for general feature extraction for monitoring data from different production lines. Second, a few supervised data are used to fine-turning a working condition classifier composed of a one-dimensional Convolutional Neural Network to verify the performance of the model in the new production line. The proposed method reduces the amount of labeled training data while at the same time enhancing the accuracy. The experiment result shows a better performance of the proposed method, which suggests a strong potential for applications in pattern recognition during sheet metal forming processes.},
   author = {Yumeng Zhu and Yanyang Zi and Jing Xu},
   doi = {10.1109/ICPHM53196.2022.9815720},
   isbn = {9781665466158},
   journal = {2022 IEEE International Conference on Prognostics and Health Management, ICPHM 2022},
   keywords = {data dimensionality reduce,industrial data processing,stacked auto-encoder,transfer learning},
   pages = {167-172},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Transfer Learning-based SAE-CNN for Industrial Data Processing in Multiple working Conditions Recognition},
   year = {2022},
}
@article{Rahman2016,
   abstract = {Unnatural process variation (UPV) is vital in quality problem of a metalstamping process. It is a major contributor to a poor quality product. The sources of UPV usually found from special causes. Recently, there is still debated among researchers in finding an effective technique for on-line monitoring-diagnosis the sources of UPV. Control charts pattern recognition (CCPR) is the most investigated technique. The existing CCPR schemes were mainly developed using raw data-based artificial neural network (ANN) recognizer, whereby the process samples were mainly generated artificially using mathematical equations. This is because the real process samples were commonly confidential or not economically available. In this research, the statistical features - ANN recognizer was utilized as the control chart pattern recognizer, whereby process sample was taken directly from an actual manufacturing process. Based on dynamic data training, the proposed recognizer has resulted in better monitoring-diagnosis performance (Normal = 100%, Unnatural = 100%) compared to the raw data- ANN (Normal = 66.67%, Unnatural = 26.97%).},
   author = {Norasulaini Abdul Rahman and Ibrahim Masood and Mohd Nasrull Abdol Rahman},
   doi = {10.1088/1757-899X/160/1/012006},
   issn = {1757899X},
   issue = {1},
   journal = {IOP Conference Series: Materials Science and Engineering},
   month = {12},
   publisher = {Institute of Physics Publishing},
   title = {Recognition of unnatural variation patterns in metal-stamping process using artificial neural network and statistical features},
   volume = {160},
   year = {2016},
}
@article{Tatipala2020,
   abstract = {The ability to predict and control the outcome of the sheet metal forming process demands holistic knowledge of the product/process parameter influences and their contribution in shaping the output product quality. Recent improvements in the ability to harvest in-line production data and the increased capability to understand complex process behaviour through computer simulations open up the possibility for new approaches to monitor and control production process performance and output product quality. This research presents an overview of the common process monitoring and control approaches while highlighting their limitations in handling the dynamics of the sheet metal forming process. The current paper envisions the need for a collaborative monitoring and control system for enhancing production process performance. Such a system must incorporate comprehensive knowledge regarding process behaviour and parameter influences in addition to the current-system-state derived using in-line production data to function effectively. Accordingly, a framework for monitoring and control within automotive sheet metal forming is proposed. The framework addresses the current limitations through the use of real-time production data and reduced process models. Lastly, the significance of the presented framework in transitioning to the digital manufacturing paradigm is reflected upon.},
   author = {Sravan Tatipala and Johan Wall and Christian Johansson and Tobias Larsson},
   doi = {10.3390/PR8010089},
   issn = {22279717},
   issue = {1},
   journal = {Processes},
   keywords = {In-line measurement data,Industry 4.0,Modelling and simulation,Process monitoring and control,Process performance,Product quality,Sheet metal forming},
   month = {1},
   publisher = {MDPI AG},
   title = {A hybrid data-based and model-based approach to process monitoring and control in sheet metal forming},
   volume = {8},
   year = {2020},
}
@article{Zhang2018,
   abstract = {Punching is a common sheet metal fabricating process for many industrial applications. On-line health condition monitoring of punching process is becoming more and more important in order to detect and correct process failures in time, and ensure the consistence of the product quality. Effective feature extraction of the process is critical for monitoring the punching process. In this work, piezoelectric strain sensors are used to measure the strain on press column surface which is the response of the press to the stamping force. A feature extraction approach based on the wavelet transform and the energy distribution of the reconstructed wavelet coefficients is proposed. The energy distribution is used as the process feature for similarity distance calculation for the process clustering. Semi-supervised clustering is applied to the process monitoring considering that many normal data sets are available while the failure data is difficult to obtain in practice. The proposed method is applied to the column strain signals from punching process and simulation failure data. The results show that combining the wavelet energy distribution of the strain signal and the semi-supervised clustering is an effective method for health condition monitoring and failure detection in punching processes.},
   author = {Guicai Zhang and Changle Li and Haitao Zhou and Timothy Wagner},
   doi = {10.1016/J.PROMFG.2018.07.156},
   issn = {23519789},
   keywords = {Punching process,failure detection,machine learning,on-line monitoring,semi-supervised clustering,wavelet transform},
   pages = {1204-1212},
   publisher = {Elsevier B.V.},
   title = {Punching process monitoring using wavelet transform based feature extraction and semi-supervised clustering},
   volume = {26},
   year = {2018},
}
@article{Yuan2020,
   abstract = {Soft sensors have been extensively used to predict difficult-to-measure quality variables for effective modeling, control and optimization of industrial processes. To construct accurate soft sensors, it is significant to carry out feature extraction for massive high-dimensional process data. Recently, deep learning has been introduced for feature representation in process data modeling. However, most of them cannot capture deep quality-related features for output prediction. In this paper, a hybrid variable-wise weighted stacked autoencoder (HVW-SAE) is developed to learn quality-related features for soft sensor modeling. By measuring the linear Pearson and nonlinear Spearman correlations for variables at the input layer with the quality variable at each encoder, a corresponding weighted reconstruction objective function is designed to successively pretrain the deep networks. With the constraint of preferential reconstruction for more quality-related variables, it can ensure that the learned features contain more information for quality prediction. Finally, the effectiveness of the proposed HVW-SAE based soft sensor method is validated on an industrial debutanizer column process.},
   author = {Xiaofeng Yuan and Chen Ou and Yalin Wang and Chunhua Yang and Weihua Gui},
   doi = {10.1016/J.NEUCOM.2018.11.107},
   issn = {18728286},
   journal = {Neurocomputing},
   keywords = {Deep learning,Feature representation,Hybrid variable-wise weighted SAE (HVW-SAE),Soft sensor,Stacked autoencoder (SAE)},
   month = {7},
   pages = {375-382},
   publisher = {Elsevier B.V.},
   title = {Deep quality-related feature extraction for soft sensing modeling: A deep learning approach with hybrid VW-SAE},
   volume = {396},
   year = {2020},
}
@article{Weimer2016,
   abstract = {Fast and reliable industrial inspection is a main challenge in manufacturing scenarios. However, the defect detection performance is heavily dependent on manually defined features for defect representation. In this contribution, we investigate a new paradigm from machine learning, namely deep machine learning by examining design configurations of deep Convolutional Neural Networks (CNN) and the impact of different hyper-parameter settings towards the accuracy of defect detection results. In contrast to manually designed image processing solutions, deep CNN automatically generate powerful features by hierarchical learning strategies from massive amounts of training data with a minimum of human interaction or expert process knowledge. An application of the proposed method demonstrates excellent defect detection results with low false alarm rates.},
   author = {Daniel Weimer and Bernd Scholz-Reiter and Moshe Shpitalni},
   doi = {10.1016/j.cirp.2016.04.072},
   issn = {17260604},
   issue = {1},
   journal = {CIRP Annals - Manufacturing Technology},
   keywords = {Artificial intelligence,Deep machine learning,Quality assurance},
   pages = {417-420},
   publisher = {Elsevier USA},
   title = {Design of deep convolutional neural network architectures for automated feature extraction in industrial inspection},
   volume = {65},
   year = {2016},
}
@article{Staar2019,
   abstract = {Over the recent years Convolutional Neural Networks (CNN) have become the primary choice for many image-processing problems. Regarding industrial applications, they are hence especially interesting for automated optical quality inspection. However, with well-optimized processes is it often not possible to obtain a sufficiently large set of defective samples for CNN-based classification and the training objective shifts from defect classification to anomaly detection. Here we approach this problem with deep metric learning using triplet networks. Our evaluation shows promising results that even translate to novel surface/defect classes, which were not part of the training data.},
   author = {Benjamin Staar and Michael Lütjen and Michael Freitag},
   doi = {10.1016/J.PROCIR.2019.02.123},
   issn = {2212-8271},
   journal = {Procedia CIRP},
   keywords = {Artificial intelligence,Inspection,Neural network,Pattern recognition},
   month = {1},
   pages = {484-489},
   publisher = {Elsevier},
   title = {Anomaly detection with convolutional neural networks for industrial surface inspection},
   volume = {79},
   year = {2019},
}
@article{,
   abstract = {This paper proposes an information theoretic criterion for comparing two partitions, or clusterings, of the same data set. The criterion, called variation of information (VI), measures the amount of information lost and gained in changing from clustering C to clustering C′. The basic properties of VI are presented and discussed. We focus on two kinds of properties: (1) those that help one build intuition about the new criterion (in particular, it is shown the VI is a true metric on the space of clusterings), and (2) those that pertain to the comparability of VI values over different experimental conditions. As the latter properties have rarely been discussed explicitly before, other existing comparison criteria are also examined in their light. Finally we present the VI from an axiomatic point of view, showing that it is the only "sensible" criterion for comparing partitions that is both aligned to the lattice and convexely additive. As a consequence, we prove an impossibility result for comparing partitions: there is no criterion for comparing partitions that simultaneously satisfies the above two desirable properties and is bounded. © 2007 Elsevier Inc. All rights reserved.},
   author = {Marina Meilǎ},
   doi = {10.1016/J.JMVA.2006.11.013},
   issn = {0047-259X},
   issue = {5},
   journal = {Journal of Multivariate Analysis},
   keywords = {Agreement measures,Clustering,Comparing partitions,Information theory,Mutual information,Similarity measures},
   month = {5},
   pages = {873-895},
   publisher = {Academic Press},
   title = {Comparing clusterings—an information based distance},
   volume = {98},
   year = {2007},
}
@article{Duan2023,
   abstract = {The true cluster number of the dataset in practical applications is rarely known in advance. Therefore, it is necessary to use a cluster validity index to evaluate the clustering results and determine the optimal cluster number. However, the performance of existing cluster validity indices is vulnerable to various factors such as cluster shape and density. To solve the above issues, this paper proposes a new cluster validity index based on augmented non-shared nearest neighbors (ANCV). The ANCV index is based on the following principles: (1) Within-cluster compactness can be measured by the distance between the pairs of data points with fewer shared nearest neighbors. (2) The distances between the pairs of data points at the intersection of clusters can be used to estimate the between-cluster separation. On this basis, the above point pairs are further extended to their augmented non-shared nearest neighbors, thereby forming small clusters. Then, the average distance within and between these clusters is calculated respectively to estimate the within-cluster compactness and between-cluster separation. Finally, the optimal number of clusters is determined by the difference between the between-cluster separation and the within-cluster compactness. Experimental results on both 12 two-dimensional synthetic datasets and 10 real datasets from UCI have shown that the ANCV index performs the best among all compared indices.},
   author = {Xinjie Duan and Yan Ma and Yuqing Zhou and Hui Huang and Bin Wang},
   doi = {10.1016/j.eswa.2023.119784},
   issn = {09574174},
   journal = {Expert Systems with Applications},
   keywords = {Between-cluster separation,Shared nearest neighbors,Validity index,Within-cluster compactness},
   month = {8},
   publisher = {Elsevier Ltd},
   title = {A novel cluster validity index based on augmented non-shared nearest neighbors},
   volume = {223},
   year = {2023},
}
@article{Bagirov2023,
   abstract = {Finding compact and well-separated clusters in data sets is a challenging task. Most clustering algorithms try to minimize certain clustering objective functions. These functions usually reflect the intra-cluster similarity and inter-cluster dissimilarity. However, the use of such functions alone may not lead to the finding of well-separated and, in some cases, compact clusters. Therefore additional measures, called cluster validity indices, are used to estimate the true number of well-separated and compact clusters. Some of these indices are well-suited to be included into the optimization model of the clustering problem. Silhouette coefficients are among such indices. In this paper, a new optimization model of the clustering problem is developed where the clustering function is used as an objective and silhouette coefficients are used to formulate constraints. Then an algorithm, called CLUSCO (CLustering Using Silhouette COefficients), is designed to construct clusters incrementally. Three schemes are discussed to reduce the computational complexity of the algorithm. Its performance is evaluated using fourteen real-world data sets and compared with that of three state-of-the-art clustering algorithms. Results show that the CLUSCO is able to compute compact clusters which are significantly better separable in comparison with those obtained by other algorithms.},
   author = {Adil M. Bagirov and Ramiz M. Aliguliyev and Nargiz Sultanova},
   doi = {10.1016/j.patcog.2022.109144},
   issn = {00313203},
   journal = {Pattern Recognition},
   keywords = {Cluster analysis,Cluster validity index,Incremental algorithm,Nonsmooth optimization,Silhouette coefficients},
   month = {3},
   publisher = {Elsevier Ltd},
   title = {Finding compact and well-separated clusters: Clustering using silhouette coefficients},
   volume = {135},
   year = {2023},
}
@article{Chen2021,
   abstract = {The comprehensive intelligent development of the manufacturing industry puts forward new requirements for the quality inspection of industrial products. This paper summarizes the current research status of machine learning methods in surface defect detection, a key part in the quality inspection of industrial products. First, according to the use of surface features, the application of traditional machine vision surface defect detection methods in industrial product surface defect detection is summarized from three aspects: texture features, color features, and shape features. Secondly, the research status of industrial product surface defect detection based on deep learning technology in recent years is discussed from three aspects: supervised method, unsupervised method, and weak supervised method. Then, the common key problems and their solutions in industrial surface defect detection are systematically summarized; the key problems include real-time problem, small sample problem, small target problem, unbalanced sample problem. Lastly, the commonly used datasets of industrial surface defects in recent years are more comprehensively summarized, and the latest research methods on the MVTec AD dataset are compared, so as to provide some reference for the further research and development of industrial surface defect detection technology.},
   author = {Yajun Chen and Yuanyuan Ding and Fan Zhao and Erhu Zhang and Zhangnan Wu and Linhao Shao},
   doi = {10.3390/APP11167657},
   issn = {2076-3417},
   issue = {16},
   journal = {Applied Sciences 2021, Vol. 11, Page 7657},
   keywords = {deep learning,defect detection,image dataset,industrial products,unbalanced samples},
   month = {8},
   pages = {7657},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Surface Defect Detection Methods for Industrial Products: A Review},
   volume = {11},
   url = {https://www.mdpi.com/2076-3417/11/16/7657/htm https://www.mdpi.com/2076-3417/11/16/7657},
   year = {2021},
}
@article{Rippel2020,
   abstract = {Anomaly Detection (AD) in images is a fundamental computer vision problem and refers to identifying images and/or image substructures that deviate significantly from the norm. Popular AD algorithms commonly try to learn a model of normality from scratch using task specific datasets, but are limited to semi-supervised approaches employing mostly normal data due to the inaccessibility of anomalies on a large scale combined with the ambiguous nature of anomaly appearance. We follow an alternative approach and demonstrate that deep feature representations learned by discriminative models on large natural image datasets are well suited to describe normality and detect even subtle anomalies in a transfer learning setting. Our model of normality is established by fitting a multivariate Gaussian (MVG) to deep feature representations of classification networks trained on ImageNet using normal data only. By subsequently applying the Mahalanobis distance as the anomaly score we outperform the current state of the art on the public MVTec AD dataset, achieving an Area Under the Receiver Operating Characteristic curve of 95.8 ± 1.2% (mean ± SEM) over all 15 classes. We further investigate why the learned representations are discriminative to the AD task using Principal Component Analysis. We find that the principal components containing little variance in normal data are the ones crucial for discriminating between normal and anomalous instances. This gives a possible explanation to the often sub-par performance of AD approaches trained from scratch using normal data only. By selectively fitting a MVG to these most relevant components only, we are able to further reduce model complexity while retaining AD performance. We also investigate setting the working point by selecting acceptable False Positive Rate thresholds based on the MVG assumption. Code is publicly available at https://github.com/ORippler/gaussian-ad-mvtec.},
   author = {Oliver Rippel and Patrick Mertens and Dorit Merhof},
   doi = {10.1109/ICPR48806.2021.9412109},
   isbn = {9781728188089},
   issn = {10514651},
   journal = {Proceedings - International Conference on Pattern Recognition},
   pages = {6726-6733},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Modeling the distribution of normal data in pre-trained deep features for anomaly detection},
   year = {2020},
}
@article{Jeong2017,
   abstract = {We propose an object detection method that improves the accuracy of the conventional SSD (Single Shot Multibox Detector), which is one of the top object detection algorithms in both aspects of accuracy and speed. The performance of a deep network is known to be improved as the number of feature maps increases. However, it is difficult to improve the performance by simply raising the number of feature maps. In this paper, we propose and analyze how to use feature maps effectively to improve the performance of the conventional SSD. The enhanced performance was obtained by changing the structure close to the classifier network, rather than growing layers close to the input data, e.g., by replacing VGGNet with ResNet. The proposed network is suitable for sharing the weights in the classifier networks, by which property, the training can be faster with better generalization power. For the Pascal VOC 2007 test set trained with VOC 2007 and VOC 2012 training sets, the proposed network with the input size of 300 × 300 achieved 78.5% mAP (mean average precision) at the speed of 35.0 FPS (frame per second), while the network with a 512 × 512 sized input achieved 80.8% mAP at 16.6 FPS using Nvidia Titan X GPU. The proposed network shows state-of-the-art mAP, which is better than those of the conventional SSD, YOLO, Faster-RCNN and RFCN. Also, it is faster than Faster-RCNN and RFCN.},
   author = {Jisoo Jeong and Hyojin Park and Nojun Kwak},
   doi = {10.5244/c.31.76},
   isbn = {190172560X},
   journal = {British Machine Vision Conference 2017, BMVC 2017},
   publisher = {BMVA Press},
   title = {Enhancement of SSD by concatenating feature maps for object detection},
   year = {2017},
}
@article{Roth2022,
   abstract = {Being able to spot defective parts is a critical component in large-scale industrial manufacturing. A particular challenge that we address in this work is the cold-start problem: fit a model using nominal (non-defective) example images only. While handcrafted solutions per class are possible, the goal is to build systems that work well simultaneously on many different tasks automatically. The best peforming approaches combine embeddings from ImageNet models with an outlier detection model. In this paper, we extend on this line of work and propose PatchCore, which uses a maximally representative memory bank of nominal patch-features. PatchCore offers competitive inference times while achieving state-of-the-art performance for both detection and localization. On the challenging, widely used MVTec AD benchmark PatchCore achieves an image-level anomaly detection AUROC score of up to 99.6%, more than halving the error compared to the next best competitor. We further report competitive results on two additional datasets and also find competitive results in the few samples regime. Code: github.com/amazon-research/patchcore-inspection.},
   author = {Karsten Roth and Latha Pemula and Joaquin Zepeda and Bernhard Scholkopf and Thomas Brox and Peter Gehler},
   doi = {10.1109/CVPR52688.2022.01392},
   isbn = {9781665469463},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   keywords = {Recognition: detection,Self-& semi-& meta- Vision applications and systems,categorization,retrieval},
   pages = {14298-14308},
   publisher = {IEEE Computer Society},
   title = {Towards Total Recall in Industrial Anomaly Detection},
   volume = {2022-June},
   year = {2022},
}
@article{Reiss2021,
   abstract = {Anomaly detection methods require high-quality features. In recent years, the anomaly detection community has attempted to obtain better features using advances in deep self-supervised feature learning. Surprisingly, a very promising direction, using pre-trained deep features, has been mostly overlooked. In this paper, we first empirically establish the perhaps expected, but unreported result, that combining pre-trained features with simple anomaly detection and segmentation methods convincingly outperforms, much more complex, state-of-the-art methods. In order to obtain further performance gains in anomaly detection, we adapt pre-trained features to the target distribution. Although transfer learning methods are well established in multi-class classification problems, the one-class classification (OCC) setting is not as well explored. It turns out that naive adaptation methods, which typically work well in supervised learning, often result in catastrophic collapse (feature deterioration) and reduce performance in OCC settings. A popular OCC method, DeepSVDD, advocates using specialized architectures, but this limits the adaptation performance gain. We propose two methods for combating collapse: i) a variant of early stopping that dynamically learns the stopping iteration ii) elastic regularization inspired by continual learning. Our method, PANDA, outperforms the state-of-the-art in the OCC, outlier exposure and anomaly segmentation settings by large margins.},
   author = {Tal Reiss and Niv Cohen and Liron Bergman and Yedid Hoshen},
   doi = {10.1109/CVPR46437.2021.00283},
   isbn = {9781665445092},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   pages = {2805-2813},
   publisher = {IEEE Computer Society},
   title = {PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation},
   year = {2021},
}
@article{Rasheed2020,
   abstract = {There are different applications of computer vision and digital image processing in various applied domains and automated production process. In textile industry, fabric defect detection is considered as a challenging task as the quality and the price of any textile product are dependent on the efficiency and effectiveness of the automatic defect detection. Previously, manual human efforts are applied in textile industry to detect the defects in the fabric production process. Lack of concentration, human fatigue, and time consumption are the main drawbacks associated with the manual fabric defect detection process. Applications based on computer vision and digital image processing can address the abovementioned limitations and drawbacks. Since the last two decades, various computer vision-based applications are proposed in various research articles to address these limitations. In this review article, we aim to present a detailed study about various computer vision-based approaches with application in textile industry to detect fabric defects. The proposed study presents a detailed overview of histogram-based approaches, color-based approaches, image segmentation-based approaches, frequency domain operations, texture-based defect detection, sparse feature-based operation, image morphology operations, and recent trends of deep learning. The performance evaluation criteria for automatic fabric defect detection is also presented and discussed. The drawbacks and limitations associated with the existing published research are discussed in detail, and possible future research directions are also mentioned. This research study provides comprehensive details about computer vision and digital image processing applications to detect different types of fabric defects.},
   author = {Aqsa Rasheed and Bushra Zafar and Amina Rasheed and Nouman Ali and Muhammad Sajid and Saadat Hanif Dar and Usman Habib and Tehmina Shehryar and Muhammad Tariq Mahmood},
   doi = {10.1155/2020/8189403},
   issn = {15635147},
   journal = {Mathematical Problems in Engineering},
   publisher = {Hindawi Limited},
   title = {Fabric Defect Detection Using Computer Vision Techniques: A Comprehensive Review},
   volume = {2020},
   year = {2020},
}
@article{Yang2020,
   abstract = {The detection of product defects is essential in quality control in manufacturing. This study surveys stateoftheart deep-learning methods in defect detection. First, we classify the defects of products, such as electronic components, pipes, welded parts, and textile materials, into categories. Second, recent mainstream techniques and deep-learning methods for defects are reviewed with their characteristics, strengths, and shortcomings described. Third, we summarize and analyze the application of ultrasonic testing, filtering, deep learning, machine vision, and other technologies used for defect detection, by focusing on three aspects, namely method and experimental results. To further understand the difficulties in the field of defect detection, we investigate the functions and characteristics of existing equipment used for defect detection. The core ideas and codes of studies related to high precision, high positioning, rapid detection, small object, complex background, occluded object detection and object association, are summarized. Lastly, we outline the current achievements and limitations of the existing methods, along with the current research challenges, to assist the research community on defect detection in setting a further agenda for future studies.},
   author = {Jing Yang and Shaobo Li and Zheng Wang and Hao Dong and Jun Wang and Shihao Tang},
   doi = {10.3390/MA13245755},
   issn = {19961944},
   issue = {24},
   journal = {Materials},
   keywords = {Deep learning,Defect detection,Object detection,Quality control},
   month = {12},
   pages = {1-23},
   publisher = {MDPI AG},
   title = {Using deep learning to detect defects in manufacturing: A comprehensive survey and current challenges},
   volume = {13},
   year = {2020},
}
@article{Jain2022,
   abstract = {Deep learning techniques, especially Convolutional Neural Networks (CNN), dominate the benchmarks for most computer vision tasks. These state-of-the-art results are typically obtained through supervised learning, for which large annotated datasets are required. However, acquiring such datasets for manufacturing applications remains a challenging proposition due to the time and costs involved in their collection. To overcome this disadvantage, a novel framework is proposed for data augmentation by creating synthetic images using Generative Adversarial Networks (GANs). The generator synthesizes new surface defect images from random noise which is trained over time to get realistic fakes. These synthetic images can be used further for training of classification algorithms. Three GAN architectures are trained, and the entire data augmentation pipeline is implemented for the Northeastern University (China) Classification (NEU-CLS) dataset for hot-rolled steel strips from NEU Surface Defect Database. The classification accuracy of a simple CNN architecture is measured on synthetic augmented data and further it is compared with similar state-of-the-arts. It is observed that the proposed GANs-based augmentation scheme significantly improves the performance of CNN for classification of surface defects. The classically augmented CNN yields sensitivity and specificity of 90.28% and 98.06% respectively. In contrast, the synthetically augmented CNN yields better results, with sensitivity and specificity of 95.33% and 99.16% respectively. Also, the use of GANs is demonstrated to disentangle the representation space and to add additional domain knowledge through synthetic augmentation that can be difficult to replicate through classic augmentation. The proposed framework demonstrates high generalization capability. It may be applied to other supervised surface inspection tasks, and thus facilitate the development of advanced vision-based inspection instruments for manufacturing applications.},
   author = {Saksham Jain and Gautam Seth and Arpit Paruthi and Umang Soni and Girish Kumar},
   doi = {10.1007/S10845-020-01710-X},
   issn = {15728145},
   issue = {4},
   journal = {Journal of Intelligent Manufacturing},
   keywords = {Classification,Convolutional neural network,Deep learning,Generative adversarial network,Surface defects},
   month = {4},
   pages = {1007-1020},
   publisher = {Springer},
   title = {Synthetic data augmentation for surface defect detection and classification using deep learning},
   volume = {33},
   year = {2022},
}
@article{Wang2021,
   abstract = {Anomaly detection is a challenging task and usually formulated as an one-class learning problem for the unexpectedness of anomalies. This paper proposes a simple yet powerful approach to this issue, which is implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and efficiency. Given a strong model pre-trained on image classification as the teacher, we distill the knowledge into a single student network with the identical architecture to learn the distribution of anomaly-free images and this one-step transfer preserves the crucial clues as much as possible. Moreover, we integrate the multi-scale feature matching strategy into the framework, and this hierarchical feature matching enables the student network to receive a mixture of multi-level knowledge from the feature pyramid under better supervision, thus allowing to detect anomalies of various sizes. The difference between feature pyramids generated by the two networks serves as a scoring function indicating the probability of anomaly occurring. Due to such operations, our approach achieves accurate and fast pixel-level anomaly detection. Very competitive results are delivered on the MVTec anomaly detection dataset, superior to the state of the art ones.},
   author = {Guodong Wang and Shumin Han and Errui Ding and Di Huang},
   month = {3},
   title = {Student-Teacher Feature Pyramid Matching for Anomaly Detection},
   url = {http://arxiv.org/abs/2103.04257},
   year = {2021},
}
@article{Wang2019,
   abstract = {Machine vision based product inspection methods have been widely investigated to improve product quality and reduce labour costs. Recent advancement in deep learning provides advanced analytics tools with high inspection accuracy and robustness. However, the construction of deep learning model is typically computationally expensive, which may not match the requirements for quick inspection. Therefore, this paper presents a new deep learning based machine vision inspection method to identify and classify defective product without loss of accuracy. In specific, Gaussian filter is first performed on the acquired image to limit random noise. Then, a region of interest (ROI) extracting project is conducted based on Hough transform to remove the unrelated background, thereby offloading the computational burden of the subsequent identification process. The construction of the identification module is based on convolutional neural network, whereas inverted residual block is introduced as the basic block to strike a good balance between identification accuracy and computational efficiency. The superior inspection performance is obtained using the proposed method with a large amount of dataset which consists of defective and defect-free bottle images.},
   author = {Jinjiang Wang and Peilun Fu and Robert X. Gao},
   doi = {10.1016/J.JMSY.2019.03.002},
   issn = {02786125},
   journal = {Journal of Manufacturing Systems},
   keywords = {Deep learning,Defective product inspection,Hough transform,Inverted residual block,Machine vision},
   month = {4},
   pages = {52-60},
   publisher = {Elsevier B.V.},
   title = {Machine vision intelligence for product defect inspection based on deep learning and Hough transform},
   volume = {51},
   year = {2019},
}
@article{Defard2020,
   abstract = {We present a new framework for Patch Distribution Modeling, PaDiM, to
concurrently detect and localize anomalies in images in a one-class learning
setting. PaDiM makes use of a pretrained convolutional neural network (CNN) for
patch embedding, and of multivariate Gaussian distributions to get a
probabilistic representation of the normal class. It also exploits correlations
between the different semantic levels of CNN to better localize anomalies.
PaDiM outperforms current state-of-the-art approaches for both anomaly
detection and localization on the MVTec AD and STC datasets. To match
real-world visual industrial inspection, we extend the evaluation protocol to
assess performance of anomaly localization algorithms on non-aligned dataset.
The state-of-the-art performance and low complexity of PaDiM make it a good
candidate for many industrial applications.},
   author = {Thomas Defard and Aleksandr Setkov and Angelique Loesch and Romaric Audigier},
   doi = {10.1007/978-3-030-68799-1_35},
   isbn = {9783030687984},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Anomaly detection,Anomaly localization,Computer vision},
   month = {11},
   pages = {475-489},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {PaDiM: a Patch Distribution Modeling Framework for Anomaly Detection and Localization},
   volume = {12664 LNCS},
   url = {https://arxiv.org/abs/2011.08785v1},
   year = {2020},
}
@article{Benyahia2022,
   abstract = {For various forms of skin lesion, many different feature extraction methods have been investigated so far. Indeed, feature extraction is a crucial step in machine learning processes. In general, we can distinct handcrafted and deep learning features. In this paper, we investigate the efficiency of using 17 commonly pre-trained convolutional neural networks (CNN) architectures as feature extractors and of 24 machine learning classifiers to evaluate the classification of skin lesions from two different datasets: ISIC 2019 and PH2. In this research, we find out that a DenseNet201 combined with Fine KNN or Cubic SVM achieved the best results in accuracy (92.34% and 91.71%) for the ISIC 2019 dataset. The results also show that the suggested method outperforms others approaches with an accuracy of 99% on the PH2 dataset.},
   author = {Samia Benyahia and Boudjelal Meftah and Olivier Lézoray},
   doi = {10.1016/J.TICE.2021.101701},
   issn = {0040-8166},
   journal = {Tissue and Cell},
   keywords = {Classification,Convolutional neural networks,Dermoscopy images,Feature extraction,Skin lesion},
   month = {2},
   pages = {101701},
   pmid = {34861582},
   publisher = {Churchill Livingstone},
   title = {Multi-features extraction based on deep learning for skin lesion classification},
   volume = {74},
   year = {2022},
}
@article{Barata2019,
   abstract = {Dermoscopy image analysis (DIA) is a growing field, with works being published every week. This makes it difficult not only to keep track of all the contributions, but also for new researchers to identify relevant information and new directions to be explored. Several surveys have been written in the past decade, but these tend to cover all of the steps of a CAD system, which can be overwhelming. Moreover, in these works, each of the steps is briefly discussed due to lack of space. Among the different blocks of the CAD system, the most relevant is the one devoted to feature extraction. This is also the block where existing works exhibit the most variability. Therefore, we believe that it is important to review the state-of-the-art on this matter. This work thoroughly explores the several types of features that have been used in DIA. A discussion on their relevance and limitations, as well as suggestions for future research are provided.},
   author = {Catarina Barata and M. Emre Celebi and Jorge S. Marques},
   doi = {10.1109/JBHI.2018.2845939},
   issn = {21682208},
   issue = {3},
   journal = {IEEE Journal of Biomedical and Health Informatics},
   keywords = {CAD systems,Dermoscopy,feature extraction,melanoma,skin cancer},
   month = {5},
   pages = {1096-1109},
   pmid = {29994234},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Survey of Feature Extraction in Dermoscopy Image Analysis of Skin Cancer},
   volume = {23},
   year = {2019},
}
@article{Pacheco2019,
   abstract = {We describe our methods that achieved the 3rd and 4th places in tasks 1 and 2, respectively, at ISIC challenge 2019. The goal of this challenge is to provide the diagnostic for skin cancer using images and meta-data. There are nine classes in the dataset, nonetheless, one of them is an outlier and is not present on it. To tackle the challenge, we apply an ensemble of classifiers, which has 13 convolutional neural networks (CNN), we develop two approaches to handle the outlier class and we propose a straightforward method to use the meta-data along with the images. Throughout this report, we detail each methodology and parameters to make it easy to replicate our work. The results obtained are in accordance with the previous challenges and the approaches to detect the outlier class and to address the meta-data seem to be work properly.},
   author = {Andre G. C. Pacheco and Abder-Rahman Ali and Thomas Trappenberg},
   month = {9},
   title = {Skin cancer detection based on deep learning and entropy to detect outlier samples},
   url = {http://arxiv.org/abs/1909.04525},
   year = {2019},
}
@article{Saqlain2020,
   abstract = {Wafer maps contain information about various defect patterns on the wafer surface and automatic classification of these defects plays a vital role to find their root causes. Semiconductor engineers apply various methods for wafer defect classification such as manual visual inspection or machine learning-based algorithms by manually extracting useful features. However, these methods are unreliable, and their classification performance is also poor. Therefore, this paper proposes a deep learning-based convolutional neural network for automatic wafer defect identification (CNN-WDI). We applied a data augmentation technique to overcome the class-imbalance issue. The proposed model uses convolution layers to extract valuable features instead of manual feature extraction. Moreover, state-of-the-art regularization methods such as batch normalization and spatial dropout are used to improve the classification performance of the CNN-WDI model. The experimental results comparison using a real wafer dataset shows that our model outperformed all previously proposed machine learning-based wafer defect classification models. The average classification accuracy of the CNN-WDI model with nine different wafer map defects is 96.2%, which is an increment of 6.4% from the last highest average accuracy using the same dataset.},
   author = {Muhammad Saqlain and Qasim Abbas and Jong Yun Lee},
   doi = {10.1109/TSM.2020.2994357},
   issn = {15582345},
   issue = {3},
   journal = {IEEE Transactions on Semiconductor Manufacturing},
   keywords = {Wafer maps,batch normalization,convolutional neural network,data augmentation,deep learning,wafer defect identification},
   month = {8},
   pages = {436-444},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Deep Convolutional Neural Network for Wafer Defect Identification on an Imbalanced Dataset in Semiconductor Manufacturing Processes},
   volume = {33},
   year = {2020},
}
@article{Kusiak2019,
   abstract = {Manufacturing is undergoing transformation driven by the developments in process technology, information technology, and data science. A future manufacturing enterprise will be highly digital. This...},
   author = {Andrew Kusiak},
   doi = {10.1080/00207543.2019.1662133},
   issn = {1366588X},
   issue = {5},
   journal = {https://doi.org/10.1080/00207543.2019.1662133},
   keywords = {convolutional neural networks,deep learning,generative adversarial networks,intelligent manufacturing,machine learning,manufacturing,smart manufacturing},
   month = {3},
   pages = {1594-1604},
   publisher = {Taylor & Francis},
   title = {Convolutional and generative adversarial neural networks in manufacturing},
   volume = {58},
   url = {https://www.tandfonline.com/doi/abs/10.1080/00207543.2019.1662133},
   year = {2019},
}
@article{Djenouri2021,
   abstract = {This article introduces a technique known as clustering with particle for object detection (CPOD) for use in smart factories. CPOD builds on regional-based methods to identify smart object data using outlier detection, clustering, particle swarm optimization (PSO), and deep convolutional networks. The process starts by removing noise and errors from the images database by the local outlier factor (LOF) algorithm. Next, the algorithm studies different correlations from the set of images in the database. This creates homogeneous, and similar clusters using the well-known k-means algorithm, and the FastRCNN (fast region convolutional neural network) uses these clusters to design efficient and more focused models. PSO is used to optimize the different parameters including, the number of neighbors of LOF, the number of clusters of k-means, the number of epochs, and the error learning rate for FastRCNN. The inference process benefits from the knowledge provided by training. Instead of considering a complex single model of the whole images database, we consider a simple homogeneous model. To demonstrate the usefulness of our approach, intensive experiments have been carried out on standard images database, and real smart manufacturer data. Our results show that CPOD when compared to baseline object detection solutions is superior in terms of runtime and accuracy.},
   author = {Youcef Djenouri and Gautam Srivastava and Jerry Chun Wei Lin},
   doi = {10.1109/TII.2020.3001493},
   issn = {19410050},
   issue = {4},
   journal = {IEEE Transactions on Industrial Informatics},
   keywords = {Clustering,deep learning,object detection,particle swarm optimization (PSO),smart factory},
   month = {4},
   pages = {2947-2955},
   publisher = {IEEE Computer Society},
   title = {Fast and Accurate Convolution Neural Network for Detecting Manufacturing Data},
   volume = {17},
   year = {2021},
}
@article{Aggarwal2001,
   abstract = {In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a efficiency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used Lknorm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric (L1 norm) is consistently more preferable than the Euclidean distance metric (L2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the Lknorm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.},
   author = {Charu C. Aggarwal and Alexander Hinneburg and Daniel A. Keim},
   doi = {10.1007/3-540-44503-X_27/COVER},
   isbn = {9783540414568},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {420-434},
   publisher = {Springer Verlag},
   title = {On the surprising behavior of distance metrics in high dimensional space},
   volume = {1973},
   url = {https://link.springer.com/chapter/10.1007/3-540-44503-x_27},
   year = {2001},
}
@article{Tang2022,
   author = {Ta-Wei Tang and Hakiem Hsu and Kuan-Ming Li},
   doi = {10.2139/SSRN.4109686},
   journal = {SSRN Electronic Journal},
   keywords = {Anomaly Detection,Feature Extraction,Hakiem Hsu,Industrial Anomaly Detection with Skip Autoencoder and Deep Feature Extractor,Industrial Defect Detection,Kuan-Ming Li,SSRN,Ta-Wei Tang,deep learning},
   month = {5},
   publisher = {Elsevier BV},
   title = {Industrial Anomaly Detection with Skip Autoencoder and Deep Feature Extractor},
   url = {https://papers.ssrn.com/abstract=4109686},
   year = {2022},
}
@article{Maschler2021,
   abstract = {The utilization of deep learning in the field of industrial automation is hindered by two factors: The amount and diversity of training data needed as well as the need to continuously retrain as the use case changes over time. Both problems can be addressed by industrial deep transfer learning allowing for the performant, continuous and potentially distributed training on small, dispersed datasets. As a specific example, a dual memory algorithm for computer vision problems is developed and evaluated. It shows the potential for state-of-the-art performance while being trained only on fractions of the complete ImageNet dataset at multiple locations at once.},
   author = {Benjamin Maschler and Simon Kamm and Michael Weyrich},
   doi = {10.1515/AUTO-2020-0119/MACHINEREADABLECITATION/RIS},
   issn = {2196677X},
   issue = {3},
   journal = {At-Automatisierungstechnik},
   keywords = {continual learning,deep learning,incremental class learning,live image recognition,transfer learning},
   month = {3},
   pages = {211-220},
   publisher = {De Gruyter Oldenbourg},
   title = {Deep industrial transfer learning at runtime for image recognition},
   volume = {69},
   url = {https://www.degruyter.com/document/doi/10.1515/auto-2020-0119/html?lang=en},
   year = {2021},
}
@article{Tercan2019,
   abstract = {In the field of production, machine learning offers great potentials to develop innovative solutions for optimization or automation. However, it faces challenges with regard to the availability of data and the high training effort of the learning models in the event of changes in the production process. In this paper, we address these challenges by introducing deep transfer learning for production. We demonstrate its potentials and benefits in a real application for predictive quality in injection molding and propose a novel approach for the continual training of neural networks across manufactured products. By creating a neural network that leverages knowledge from previous products without forgetting them, the approach shows better learning rates and more accurate predictions while requiring much less data for training. Our code is publicly available1 to reproduce our results and build upon them.},
   author = {Hasan Tercan and Alexandro Guajardo and Tobias Meisen},
   doi = {10.1109/INDIN41052.2019.8972099},
   isbn = {9781728129273},
   issn = {19354576},
   journal = {IEEE International Conference on Industrial Informatics (INDIN)},
   keywords = {Artificial intelligence,Deep learning,Domain adaptation,Engineering,Industrial,Machine learning,Manufacturing,Transfer learning},
   month = {7},
   pages = {274-279},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Industrial transfer learning: Boosting machine learning in production},
   volume = {2019-July},
   year = {2019},
}
@article{Maschler2020,
   abstract = {In this paper, a novel light-weight incremental class learning algorithm for live image recognition is presented. It features a dual memory architecture and is capable of learning formerly unknown classes as well as conducting its learning across multiple instances at multiple locations without storing any images. In addition to tests on the ImageNet dataset, a prototype based upon a Raspberry Pi and a webcam is used for further evaluation: The proposed algorithm successfully allows for the performant execution of image classification tasks while learning new classes at several sites simultaneously, thereby enabling its application to various industry use cases, e.g. predictive maintenance or self-optimization.},
   author = {Benjamin Maschler and Simon Kamm and Nasser Jazdi and Michael Weyrich},
   doi = {10.1016/J.PROCIR.2020.03.056},
   issn = {22128271},
   journal = {Procedia CIRP},
   keywords = {Artificial intelligence,Artificial neural networks,Continual learning,Deep learning,Distributed learning,Dual memory method,Incremental class learning,Live image recognition,Transfer learning},
   pages = {437-442},
   publisher = {Elsevier B.V.},
   title = {Distributed cooperative deep transfer learning for industrial image recognition},
   volume = {93},
   year = {2020},
}
@article{Maschler2021,
   abstract = {Deep learning has greatly increased the capabilities of "intelligent"technical systems over the last years [1]. This includes the industrial automation sector [1]-[4], where new data-driven approaches to, for example, predictive maintenance [2], computer vision [3], or anomaly detection [4], have resulted in systems more easily and robustly automated than ever before.},
   author = {Benjamin Maschler and Michael Weyrich},
   doi = {10.1109/MIE.2020.3034884},
   issn = {19410115},
   issue = {2},
   journal = {IEEE Industrial Electronics Magazine},
   month = {6},
   pages = {65-75},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Deep Transfer Learning for Industrial Automation: A Review and Discussion of New Techniques for Data-Driven Machine Learning},
   volume = {15},
   year = {2021},
}
@article{Wilmet2021,
   abstract = {Anomaly detection in images plays a significant role for many applications
across all industries, such as disease diagnosis in healthcare or quality
assurance in manufacturing. Manual inspection of images, when extended over a
monotonously repetitive period of time is very time consuming and can lead to
anomalies being overlooked.Artificial neural networks have proven themselves
very successful on simple, repetitive tasks, in some cases even outperforming
humans. Therefore, in this paper we investigate different methods of deep
learning, including supervised and unsupervised learning, for anomaly detection
applied to a quality assurance use case. We utilize the MVTec anomaly dataset
and develop three different models, a CNN for supervised anomaly detection,
KD-CAE for autoencoder anomaly detection, NI-CAE for noise induced anomaly
detection and a DCGAN for generating reconstructed images. By experiments, we
found that KD-CAE performs better on the anomaly datasets compared to CNN and
NI-CAE, with NI-CAE performing the best on the Transistor dataset. We also
implemented a DCGAN for the creation of new training data but due to
computational limitation and lack of extrapolating the mechanics of AnoGAN, we
restricted ourselves just to the generation of GAN based images. We conclude
that unsupervised methods are more powerful for anomaly detection in images,
especially in a setting where only a small amount of anomalous data is
available, or the data is unlabeled.},
   author = {Vincent Wilmet and Sauraj Verma and Tabea Redl and Håkon Sandaker and Zhenning Li},
   month = {7},
   title = {A Comparison of Supervised and Unsupervised Deep Learning Methods for Anomaly Detection in Images},
   url = {https://arxiv.org/abs/2107.09204v1},
   year = {2021},
}
@article{Minhas2019,
   abstract = {Humans can easily detect a defect (anomaly) because it is different or
salient when compared to the surface it resides on. Today, manual human visual
inspection is still the norm because it is difficult to automate anomaly
detection. Neural networks are a useful tool that can teach a machine to find
defects. However, they require a lot of training examples to learn what a
defect is and it is tedious and expensive to get these samples. We tackle the
problem of teaching a network with a low number of training samples with a
system we call AnoNet. AnoNet's architecture is similar to CompactCNN with the
exceptions that (1) it is a fully convolutional network and does not use
strided convolution; (2) it is shallow and compact which minimizes over-fitting
by design; (3) the compact design constrains the size of intermediate features
which allows training to be done without image downsizing; (4) the model
footprint is low making it suitable for edge computation; and (5) the anomaly
can be detected and localized despite the weak labelling. AnoNet learns to
detect the underlying shape of the anomalies despite the weak annotation as
well as preserves the spatial localization of the anomaly. Pre-seeding AnoNet
with an engineered filter bank initialization technique reduces the total
samples required for training and also achieves state-of-the-art performance.
Compared to the CompactCNN, AnoNet achieved a massive 94% reduction of network
parameters from 1.13 million to 64 thousand parameters. Experiments were
conducted on four data-sets and results were compared against CompactCNN and
DeepLabv3. AnoNet improved the performance on an average across all data-sets
by 106% to an F1 score of 0.98 and by 13% to an AUROC value of 0.942. AnoNet
can learn from a limited number of images. For one of the data-sets, AnoNet
learnt to detect anomalies after a single pass through just 53 training images.},
   author = {Manpreet Singh Minhas and John Zelek},
   month = {11},
   title = {AnoNet: Weakly Supervised Anomaly Detection in Textured Surfaces},
   url = {https://arxiv.org/abs/1911.10608v1},
   year = {2019},
}
@web_page{,
   title = {A Comparative Study of Faster R-CNN Models for Anomaly Detection in 2019 AI City Challenge | Enhanced Reader},
}
@article{Wang2022,
   abstract = {This paper proposed a novel anomaly detection (AD) approach of High-speed
Train images based on convolutional neural networks and the Vision Transformer.
Different from previous AD works, in which anomalies are identified with a
single image using classification, segmentation, or object detection methods,
the proposed method detects abnormal difference between two images taken at
different times of the same region. In other words, we cast anomaly detection
problem with a single image into a difference detection problem with two
images. The core idea of the proposed method is that the 'anomaly' usually
represents an abnormal state instead of a specific object, and this state
should be identified by a pair of images. In addition, we introduced a deep
feature difference AD network (AnoDFDNet) which sufficiently explored the
potential of the Vision Transformer and convolutional neural networks. To
verify the effectiveness of the proposed AnoDFDNet, we collected three
datasets, a difference dataset (Diff Dataset), a foreign body dataset (FB
Dataset), and an oil leakage dataset (OL Dataset). Experimental results on
above datasets demonstrate the superiority of proposed method. Source code are
available at https://github.com/wangle53/AnoDFDNet.},
   author = {Zhixue Wang and Yu Zhang and Lin Luo and Nan Wang},
   month = {3},
   title = {AnoDFDNet: A Deep Feature Difference Network for Anomaly Detection},
   url = {https://arxiv.org/abs/2203.15195v1},
   year = {2022},
}
@article{Zhang2019,
   abstract = {The Deep learning algorithm shows is strong capability in pattern recognition tasks such as object detection and speech recognition. Comparing with the typical machine learning methods which require extraction of manual features, deep learning algorithm has a more powerful feature learning ability. In this paper, the deep learning associated object detection method is applied to developed to locate and classify lesions for the detection of medical breast masses. At the same time, transfer learning based on the network of Faster RCNN is also introduced. Furthermore, five feature extractors of the network, which are ResNet101, inception V2, inception V3, Mobilenet, and inception ResNet V2, are investigated for exploring the impact for the model. Digital Database for Screening Mammography(DDSM) dataset is used in the investigation, and the performances of the models associated with the five feature extractors are compared separately in detecting benign and malignant breasts, based on the ROC trade-off curves. The simulation results demonstrate that the classification model with Inception ResNet V2 feature extractor exhibit the best performance, compared with the other four feature extractors.},
   author = {Zhen Zhang and Yaping Wang and Jiankang Zhang and Xiaomin Mu},
   doi = {10.1109/ISNE.2019.8896490},
   isbn = {9781728120621},
   journal = {2019 8th International Symposium on Next Generation Electronics, ISNE 2019},
   keywords = {Faster RCNN,breast masses,object detection,transfer learning},
   month = {10},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Comparison of multiple feature extractors on Faster RCNN for breast tumor detection},
   year = {2019},
}
@article{Gunasinghe2021,
   abstract = {Glaucoma identification is often performed manually by medical professionals, which is time consuming. Automated image analysis (e.g. of retinal fundus images) can speed up the detection and treatment of this disease. In this study, twentysix deep learning models pretrained for object recognition are compared as potential feature extractors for glaucoma detection from retinal fundus images. We used a template matching algorithm to automate the cropping around the optic nerve head at three different scales and extracted the features using pretrained networks from both cropped and full versions of the images. Cropped features were concatenated with full image features to create expanded feature sets. Next, we conducted extensive ten-fold cross-validation experiments using random forest and optimised logistic regression base classifiers to estimate the accuracy of models trained on each feature set individually and also on various combinations of feature sets from the full and cropped images. The best feature extractor for glaucoma detection was the residual network (ResNet) giving 0.97 cross validated AUC-ROC in conjunction with the random forest classifier using concatenated features. The experimental results indicate that ResNet architecture is best suited as a feature extractor for glaucoma identification from retinal fundus images, but only if two feature sets (full and cropped) are utilised.},
   author = {Hansi Gunasinghe and James McKelvie and Abigail Koay and Michael Mayo},
   doi = {10.1109/ISBI48211.2021.9434082},
   isbn = {9781665412469},
   issn = {19458452},
   journal = {Proceedings - International Symposium on Biomedical Imaging},
   keywords = {Feature extraction,Glaucoma detection,Machine learning,ResNet,Retinal fundus images},
   month = {4},
   pages = {390-394},
   publisher = {IEEE Computer Society},
   title = {Comparison of pretrained feature extractors for glaucoma detection},
   volume = {2021-April},
   year = {2021},
}
@web_page{Hinterstoisser2018,
   author = {Stefan Hinterstoisser and Vincent Lepetit and Paul Wohlhart and Kurt Konolige},
   pages = {0-0},
   title = {On Pre-Trained Image Features and Synthetic Images for Deep Learning},
   year = {2018},
}
@article{Hinterstoisser2017,
   abstract = {Deep Learning methods usually require huge amounts of training data to
perform at their full potential, and often require expensive manual labeling.
Using synthetic images is therefore very attractive to train object detectors,
as the labeling comes for free, and several approaches have been proposed to
combine synthetic and real images for training. In this paper, we show that a simple trick is sufficient to train very
effectively modern object detectors with synthetic images only: We freeze the
layers responsible for feature extraction to generic layers pre-trained on real
images, and train only the remaining layers with plain OpenGL rendering. Our
experiments with very recent deep architectures for object recognition
(Faster-RCNN, R-FCN, Mask-RCNN) and image feature extractors (InceptionResnet
and Resnet) show this simple approach performs surprisingly well.},
   author = {Stefan Hinterstoisser and Vincent Lepetit and Paul Wohlhart and Kurt Konolige},
   doi = {10.1007/978-3-030-11009-3_42},
   isbn = {9783030110086},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   month = {10},
   pages = {682-697},
   publisher = {Springer Verlag},
   title = {On Pre-Trained Image Features and Synthetic Images for Deep Learning},
   volume = {11129 LNCS},
   url = {https://arxiv.org/abs/1710.10710v2},
   year = {2017},
}
@article{Grandini2020,
   abstract = {Classification tasks in machine learning involving more than two classes are known by the name of "multi-class classification". Performance indicators are very useful when the aim is to evaluate and compare different classification models or machine learning techniques. Many metrics come in handy to test the ability of a multi-class classifier. Those metrics turn out to be useful at different stage of the development process, e.g. comparing the performance of two different models or analysing the behaviour of the same model by tuning different parameters. In this white paper we review a list of the most promising multi-class metrics, we highlight their advantages and disadvantages and show their possible usages during the development of a classification model.},
   author = {Margherita Grandini and Enrico Bagli and Giorgio Visani},
   month = {8},
   title = {Metrics for Multi-Class Classification: an Overview},
   url = {http://arxiv.org/abs/2008.05756},
   year = {2020},
}
@article{Yosinski2014,
   abstract = {Many deep neural networks trained on natural images exhibit a curious
phenomenon in common: on the first layer they learn features similar to Gabor
filters and color blobs. Such first-layer features appear not to be specific to
a particular dataset or task, but general in that they are applicable to many
datasets and tasks. Features must eventually transition from general to
specific by the last layer of the network, but this transition has not been
studied extensively. In this paper we experimentally quantify the generality
versus specificity of neurons in each layer of a deep convolutional neural
network and report a few surprising results. Transferability is negatively
affected by two distinct issues: (1) the specialization of higher layer neurons
to their original task at the expense of performance on the target task, which
was expected, and (2) optimization difficulties related to splitting networks
between co-adapted neurons, which was not expected. In an example network
trained on ImageNet, we demonstrate that either of these two issues may
dominate, depending on whether features are transferred from the bottom,
middle, or top of the network. We also document that the transferability of
features decreases as the distance between the base task and target task
increases, but that transferring features even from distant tasks can be better
than using random features. A final surprising result is that initializing a
network with transferred features from almost any number of layers can produce
a boost to generalization that lingers even after fine-tuning to the target
dataset.},
   author = {Jason Yosinski and Jeff Clune and Yoshua Bengio and Hod Lipson},
   issn = {10495258},
   issue = {January},
   journal = {Advances in Neural Information Processing Systems},
   month = {11},
   pages = {3320-3328},
   publisher = {Neural information processing systems foundation},
   title = {How transferable are features in deep neural networks?},
   volume = {4},
   url = {https://arxiv.org/abs/1411.1792v1},
   year = {2014},
}
@article{Jia2022,
   abstract = {As basic research, it has also received increasing attention from people that the “curse of dimensionality” will lead to increase the cost of data storage and computing; it also influences the efficiency and accuracy of dealing with problems. Feature dimensionality reduction as a key link in the process of pattern recognition has become one hot and difficulty spot in the field of pattern recognition, machine learning and data mining. It is one of the most challenging research fields, which has been favored by most of the scholars’ attention. How to implement “low loss” in the process of feature dimension reduction, keep the nature of the original data, find out the best mapping and get the optimal low dimensional data are the keys aims of the research. In this paper, two-dimensionality reduction methods, feature selection and feature extraction, are introduced; the current mainstream dimensionality reduction algorithms are analyzed, including the method for small sample and method based on deep learning. For each algorithm, examples of their application are given and the advantages and disadvantages of these methods are evaluated.},
   author = {Weikuan Jia and Meili Sun and Jian Lian and Sujuan Hou},
   doi = {10.1007/S40747-021-00637-X},
   issn = {2198-6053},
   issue = {3},
   journal = {Complex & Intelligent Systems 2022 8:3},
   keywords = {Complexity,Computational Intelligence,Data Structures and Information Theory,Feature selection},
   month = {1},
   pages = {2663-2693},
   publisher = {Springer},
   title = {Feature dimensionality reduction: a review},
   volume = {8},
   url = {https://link.springer.com/article/10.1007/s40747-021-00637-x},
   year = {2022},
}
@article{Aljalbout2018,
   abstract = {Clustering methods based on deep neural networks have proven promising for
clustering real-world data because of their high representational power. In
this paper, we propose a systematic taxonomy of clustering methods that utilize
deep neural networks. We base our taxonomy on a comprehensive review of recent
work and validate the taxonomy in a case study. In this case study, we show
that the taxonomy enables researchers and practitioners to systematically
create new clustering methods by selectively recombining and replacing distinct
aspects of previous methods with the goal of overcoming their individual
limitations. The experimental evaluation confirms this and shows that the
method created for the case study achieves state-of-the-art clustering quality
and surpasses it in some cases.},
   author = {Elie Aljalbout and Vladimir Golkov and Yawar Siddiqui and Maximilian Strobel and Daniel Cremers},
   month = {1},
   title = {Clustering with Deep Learning: Taxonomy and New Methods},
   url = {https://arxiv.org/abs/1801.07648v2},
   year = {2018},
}
@generic{Saito2017,
   abstract = {Effective clustering can be achieved by mapping the input to an embedded space rather than clustering on the raw data itself. However, there is limited focus on transformation methods that improve clustering accuracies. In this paper, we introduce Neural Clustering, a simple yet effective unsupervised model to project data onto an embedded space where intermediate layers of a deep autoencoder are concatenated to generate high-dimensional representations. Optimization of the autoencoder via reconstruction error allows the layers in the network to learn semantic representations of different classes of data. Our experimental results yield significant improvements on other models and a robustness across different kinds of datasets.},
   author = {Sean Saito and Robby T. Tan},
   month = {2},
   title = {Neural Clustering: Concatenating Layers for Better Projections},
   year = {2017},
}
@article{Caron2018,
   abstract = {Clustering is a class of unsupervised learning methods that has been
extensively applied and studied in computer vision. Little work has been done
to adapt it to the end-to-end training of visual features on large scale
datasets. In this work, we present DeepCluster, a clustering method that
jointly learns the parameters of a neural network and the cluster assignments
of the resulting features. DeepCluster iteratively groups the features with a
standard clustering algorithm, k-means, and uses the subsequent assignments as
supervision to update the weights of the network. We apply DeepCluster to the
unsupervised training of convolutional neural networks on large datasets like
ImageNet and YFCC100M. The resulting model outperforms the current state of the
art by a significant margin on all the standard benchmarks.},
   author = {Mathilde Caron and Piotr Bojanowski and Armand Joulin and Matthijs Douze},
   doi = {10.1007/978-3-030-01264-9_9},
   isbn = {9783030012632},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Clustering,Unsupervised learning},
   month = {7},
   pages = {139-156},
   publisher = {Springer Verlag},
   title = {Deep Clustering for Unsupervised Learning of Visual Features},
   volume = {11218 LNCS},
   url = {https://arxiv.org/abs/1807.05520v2},
   year = {2018},
}
@article{Sarfraz2019,
   abstract = {We present a new clustering method in the form of a single clustering equation that is able to directly discover groupings in the data. The main proposition is that the first neighbor of each sample is all one needs to discover large chains and finding the groups in the data. In contrast to most existing clustering algorithms our method does not require any hyper-parameters, distance thresholds and/or the need to specify the number of clusters. The proposed algorithm belongs to the family of hierarchical agglomerative methods. The technique has a very low computational overhead, is easily scalable and applicable to large practical problems. Evaluation on well known datasets from different domains ranging between 1077 and 8.1 million samples shows substantial performance gains when compared to the existing clustering techniques.},
   author = {Saquib Sarfraz and Vivek Sharma and Rainer Stiefelhagen},
   doi = {10.1109/CVPR.2019.00914},
   isbn = {9781728132938},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   keywords = {Big Data,Grouping and Shape,Large Scale Methods,Others,Representation Learning,Segmentation,Vision Applications and Systems},
   month = {6},
   pages = {8926-8935},
   publisher = {IEEE Computer Society},
   title = {Efficient parameter-free clustering using first neighbor relations},
   volume = {2019-June},
   year = {2019},
}
@article{Tschuchnig2022,
   abstract = {The increasing digitization of medical imaging enables machine learning based improvements in detecting, visualizing and segmenting lesions, easing the workload for medical experts. However, supervised machine learning requires reliable labelled data, which is is often difficult or impossible to collect or at least time consuming and thereby costly. Therefore methods requiring only partly labeled data (semi-supervised) or no labeling at all (unsupervised methods) have been applied more regularly. Anomaly detection is one possible methodology that is able to leverage semi-supervised and unsupervised methods to handle medical imaging tasks like classification and segmentation. This paper uses a semi-exhaustive literature review of relevant anomaly detection papers in medical imaging to cluster into applications, highlight important results, establish lessons learned and give further advice on how to approach anomaly detection in medical imaging. The qualitative analysis is based on google scholar and 4 different search terms, resulting in 120 different analysed papers. The main results showed that the current research is mostly motivated by reducing the need for labelled data. Also, the successful and substantial amount of research in the brain MRI domain shows the potential for applications in further domains like OCT and chest X-ray.},
   author = {Maximilian E. Tschuchnig and Michael Gadermayr},
   doi = {10.1007/978-3-658-36295-9_5/COVER},
   journal = {Data Science – Analytics and Applications},
   pages = {33-38},
   publisher = {Springer Fachmedien Wiesbaden},
   title = {Anomaly Detection in Medical Imaging - A Mini Review},
   url = {https://link.springer.com/chapter/10.1007/978-3-658-36295-9_5},
   year = {2022},
}
@article{Boukerche2020,
   abstract = {Over the past decade, we have witnessed an enormous amount of research effort dedicated to the design of efficient outlier detection techniques while taking into consideration efficiency, accuracy,...},
   author = {Azzedine Boukerche and Lining Zheng and Omar Alfandi},
   doi = {10.1145/3381028},
   issn = {15577341},
   issue = {3},
   journal = {ACM Computing Surveys (CSUR)},
   keywords = {Outlier detection,anomaly detection,semi-supervised learning,unsupervised learning},
   month = {6},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Outlier Detection},
   volume = {53},
   url = {https://dl.acm.org/doi/10.1145/3381028},
   year = {2020},
}
@article{Kumar2014,
   abstract = {Distance measures play an important role in cluster analysis. There is no single distance measure that best fits for all types of the clustering problems. So, it is important to find set of distance measures for different clustering techniques on datasets that yields optimal results. In this paper, an attempt has been made to evaluate ten different distance measures on eight clustering techniques. The quality of the distance measures has been computed on basis of three factors: accuracy, inter-cluster and intra-cluster distances. The performance of clustering algorithms on different distance measures has been evaluated on three artificial and six real life datasets. The experimental results reveal that the performance and quality of different distance measures vary with the nature of data as well as clustering techniques. Hence choice of distance measure must be done on basis of dataset and clustering technique.},
   author = {Vijay Kumar and Jitender Kumar Chhabra and Dinesh Kumar},
   issn = {1982-3363},
   issue = {1},
   journal = {INFOCOMP Journal of Computer Science},
   keywords = {it is important to find set of distance measures for different clustering techniques on datasets},
   month = {9},
   pages = {38-52},
   title = {Performance Evaluation of Distance Metrics in the Clustering Algorithms},
   volume = {13},
   url = {https://infocomp.dcc.ufla.br/index.php/infocomp/article/view/21},
   year = {2014},
}
@article{Qu2023,
   abstract = {With the rapid development of science and technology, high-dimensional data have been widely used in various fields. Due to the complex characteristics of high-dimensional data, it is usually distributed in the union of several low-dimensional subspaces. In the past several decades, subspace clustering (SC) methods have been widely studied as they can restore the underlying subspace of high-dimensional data and perform fast clustering with the help of the data self-expressiveness property. The SC methods aim to construct an affinity matrix by the self-representation coefficient of high-dimensional data and then obtain the clustering results using the spectral clustering method. The key is how to design a self-expressiveness model that can reveal the real subspace structure of data. In this survey, we focus on the development of SC methods in the past two decades and present a new classification criterion to divide them into three categories based on the purpose of clustering, i.e., low-rank sparse SC, local structure preserving SC, and kernel SC. We further divide them into subcategories according to the strategy of constructing the representation coefficient. In addition, the applications of SC methods in face recognition, motion segmentation, handwritten digits recognition, and speech emotion recognition are introduced. Finally, we have discussed several interesting and meaningful future research directions.},
   author = {Wentao Qu and Xianchao Xiu and Huangyue Chen and Lingchen Kong},
   doi = {10.3390/MATH11020436},
   issn = {2227-7390},
   issue = {2},
   journal = {Mathematics 2023, Vol. 11, Page 436},
   keywords = {dimensional data,high,kernel learning,machine learning,sparse optimization,subspace clustering},
   month = {1},
   pages = {436},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {A Survey on High-Dimensional Subspace Clustering},
   volume = {11},
   url = {https://www.mdpi.com/2227-7390/11/2/436/htm https://www.mdpi.com/2227-7390/11/2/436},
   year = {2023},
}
@article{Sen2019,
   abstract = {Feature selection is one of the most important preprocessing steps in Machine Learning. This can be broadly divided into search based methods and ranking based methods. The ranking based methods are very popular because they need much lesser computational power. There can be many different ways to rank the features. One of the ways to measure effectiveness of a feature is by evaluating its ability to separate the classes involved. These interclass Separability based measures can be directly used as a feature ranking tool for binary classification problems. Bhattacharya Distance which is the most popular among them has been used majorly in a recursive setup to select good quality feature subsets. Jeffries-Matusita (JM) distance improves Bhattacharya distance by normalizing it between 0 and 2. In this paper, we have ranked the features based on JM distance. The results are comparable with mutual information, Relief and Chi Squared based measures as per experiments conducted over 24 public datasets but in much lesser time. JM distance also provide some intuition about the dataset prior to any feature selection or machine learning algorithm. A comparison has been done on classification accuracy and JM scores of these datasets, which can provide a good intuition on how good a dataset is for classification and point out the need of or lack of further feature collection.},
   author = {Rikta Sen and Saptarsi Goswami and Basabi Chakraborty},
   doi = {10.1109/ICDSE47409.2019.8971800},
   isbn = {9781728120874},
   journal = {2019 International Conference on Data Science and Engineering, ICDSE 2019},
   keywords = {Bhattacharya Distance,Chi Squared,Feature selection,Jeffries-Matusita (JM) distance,Mutual information,Relief},
   month = {9},
   pages = {15-20},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Jeffries-Matusita distance as a tool for feature selection},
   year = {2019},
}
@article{,
   abstract = {Clustering evaluation plays an important role in unsupervised learning systems, as it is often necessary to automatically quantify the quality of generated cluster configurations. This is especially useful for comparing the performance of different clustering algorithms as well as determining the optimal number of clusters in clustering algorithms that do not estimate it internally. Many clustering quality indexes have been proposed over the years and different indexes are used in different contexts. There is no unifying protocol for clustering evaluation, so it is often unclear which quality index to use in which case. In this chapter, we review the existing clustering quality measures and evaluate them in the challenging context of high-dimensional data clustering. High-dimensional data is sparse and distances tend to concentrate, possibly affecting the applicability of various clustering quality indexes. We analyze the stability and discriminative power of a set of standard clustering quality measures with increasing data dimensionality. Our evaluation shows that the curse of dimensionality affects different clustering quality indexes in different ways and that some are to be preferred when determining clustering quality in many dimensions.},
   author = {Nenad Tomašev and Miloš Radovanović},
   doi = {10.1007/978-3-319-24211-8_4},
   isbn = {9783319242118},
   journal = {Unsupervised Learning Algorithms},
   month = {1},
   pages = {71-107},
   publisher = {Springer International Publishing},
   title = {Clustering evaluation in high-dimensional data},
   url = {https://www.researchgate.net/publication/301736217_Clustering_Evaluation_in_High-Dimensional_Data},
   year = {2016},
}
